# SQUAD ONE - ULTRA COMPACT EXPORT
# Files: 12, Size: 265096 bytes

=== FILE: app.py ===
import os
import logging
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.orm import DeclarativeBase
from werkzeug.middleware.proxy_fix import ProxyFix

# Set up logging
logging.basicConfig(level=logging.DEBUG)

class Base(DeclarativeBase):
    pass

db = SQLAlchemy(model_class=Base)

# Create the app
app = Flask(__name__)
app.secret_key = os.environ.get("SESSION_SECRET", "dev-secret-key-change-in-production")
app.wsgi_app = ProxyFix(app.wsgi_app, x_proto=1, x_host=1)

# Configure the database
app.config["SQLALCHEMY_DATABASE_URI"] = os.environ.get("DATABASE_URL", "sqlite:///chat.db")
app.config["SQLALCHEMY_ENGINE_OPTIONS"] = {
    "pool_recycle": 300,
    "pool_pre_ping": True,
}

# Initialize the app with the extension
db.init_app(app)

with app.app_context():
    # Import models to ensure tables are created
    import models
    db.create_all()

# Import routes
from routes import *

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)


=== END FILE ===

=== FILE: main.py ===
from app import app

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)


=== END FILE ===

=== FILE: models.py ===
from app import db
from datetime import datetime

class ChatMessage(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    session_id = db.Column(db.String(64), nullable=False, index=True)
    role = db.Column(db.String(20), nullable=False)  # 'user' or 'assistant'
    content = db.Column(db.Text, nullable=False)
    timestamp = db.Column(db.DateTime, default=datetime.utcnow)
    
    def to_dict(self):
        return {
            'id': self.id,
            'session_id': self.session_id,
            'role': self.role,
            'content': self.content,
            'timestamp': self.timestamp.isoformat()
        }


=== END FILE ===

=== FILE: routes.py ===
import os
import uuid
import time
import logging
from flask import render_template, request, jsonify, session
from app import app, db
from models import ChatMessage

# Try to use Gemini first, then fallback to transformers model, then mock
model_service = None

try:
    from gemini_service import GeminiService
    model_service = GeminiService()
    logging.info("Using Gemini AI service")
except Exception as e:
    logging.warning(f"Gemini service failed: {e}")
    try:
        from model_service import ModelService
        model_service = ModelService()
        # Check if the real model service has transformers available
        if model_service.error and "transformers" in model_service.error.lower():
            raise ImportError("Transformers not available")
        logging.info("Using HuggingFace transformers model service")
    except ImportError:
        # Fall back to mock service for demonstration
        logging.warning("Using mock model service - install torch and transformers for real AI model")
        from mock_model_service import MockModelService
        model_service = MockModelService()

@app.route('/')
def index():
    """Main chat interface"""
    if 'session_id' not in session:
        session['session_id'] = str(uuid.uuid4())
    return render_template('index.html')

@app.route('/react')
def react_interface():
    """React Spectrum UI interface"""
    if 'session_id' not in session:
        session['session_id'] = str(uuid.uuid4())
    return render_template('react.html')

@app.route('/build-squad')
def build_squad():
    """Build Squad - Project Chimera interface"""
    return render_template('build_squad.html')

@app.route('/the-isp')
def the_isp():
    """THE ISP - Avatar Conversation Prototype Interface"""
    # Pass API configuration to frontend
    api_config = {
        'livekit_url': os.environ.get('LIVEKIT_URL', ''),
        'has_livekit_credentials': bool(os.environ.get('LIVEKIT_API_KEY') and os.environ.get('LIVEKIT_API_SECRET')),
        'has_huggingface_token': bool(os.environ.get('HUGGINGFACE_TOKEN')),
        'has_gemini_key': bool(os.environ.get('GEMINI_API_KEY')),
        'has_github_token': bool(os.environ.get('GITHUB_TOKEN')),
        'has_docker_key': bool(os.environ.get('DOCKER_API_KEY')),
        'has_csm_model': False,  # Will be updated by frontend check
        'has_livekit_full': bool(os.environ.get('LIVEKIT_API_KEY') and os.environ.get('LIVEKIT_API_SECRET') and os.environ.get('LIVEKIT_URL'))
    }
    return render_template('the_isp.html', api_config=api_config)

@app.route('/deployment')
def deployment_page():
    """Deployment generator interface"""
    return render_template('deployment.html')

@app.route('/mcp-setup')
def mcp_setup():
    """MCP Setup Guide for THE ISP"""
    return render_template('mcp_setup.html')

@app.route('/api/chat', methods=['POST'])
def chat():
    """Handle chat messages"""
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({'error': 'No message provided'}), 400
        
        user_message = data['message'].strip()
        if not user_message:
            return jsonify({'error': 'Empty message'}), 400
        
        session_id = session.get('session_id', str(uuid.uuid4()))
        
        # Save user message
        user_msg = ChatMessage()
        user_msg.session_id = session_id
        user_msg.role = 'user'
        user_msg.content = user_message
        db.session.add(user_msg)
        db.session.commit()
        
        # Get model response
        try:
            assistant_response = model_service.generate_response(user_message, session_id)
        except Exception as e:
            logging.error(f"Model generation error: {str(e)}")
            return jsonify({'error': f'Model generation failed: {str(e)}'}), 500
        
        # Save assistant message
        assistant_msg = ChatMessage()
        assistant_msg.session_id = session_id
        assistant_msg.role = 'assistant'
        assistant_msg.content = assistant_response
        db.session.add(assistant_msg)
        db.session.commit()
        
        return jsonify({
            'user_message': user_msg.to_dict(),
            'assistant_message': assistant_msg.to_dict()
        })
        
    except Exception as e:
        logging.error(f"Chat error: {str(e)}")
        return jsonify({'error': f'Server error: {str(e)}'}), 500

@app.route('/api/history')
def get_history():
    """Get chat history for current session"""
    try:
        session_id = session.get('session_id')
        if not session_id:
            return jsonify({'messages': []})
        
        messages = ChatMessage.query.filter_by(session_id=session_id).order_by(ChatMessage.timestamp).all()
        return jsonify({'messages': [msg.to_dict() for msg in messages]})
        
    except Exception as e:
        logging.error(f"History error: {str(e)}")
        return jsonify({'error': f'Failed to load history: {str(e)}'}), 500

@app.route('/api/clear', methods=['POST'])
def clear_chat():
    """Clear chat history for current session"""
    try:
        session_id = session.get('session_id')
        if session_id:
            ChatMessage.query.filter_by(session_id=session_id).delete()
            db.session.commit()
        
        # Generate new session ID
        session['session_id'] = str(uuid.uuid4())
        
        return jsonify({'success': True})
        
    except Exception as e:
        logging.error(f"Clear chat error: {str(e)}")
        return jsonify({'error': f'Failed to clear chat: {str(e)}'}), 500

@app.route('/api/status')
def model_status():
    """Check model loading status"""
    try:
        status = model_service.get_status()
        return jsonify(status)
    except Exception as e:
        logging.error(f"Status error: {str(e)}")
        return jsonify({'error': f'Failed to get status: {str(e)}'}), 500

@app.route('/api/hf-chat', methods=['POST'])
def hf_chat():
    """HuggingFace chat endpoint for THE ISP avatar conversations"""
    try:
        import requests
        
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({'error': 'No message provided'}), 400
        
        user_message = data['message'].strip()
        if not user_message:
            return jsonify({'error': 'Empty message'}), 400
        
        # Get HuggingFace token
        hf_token = os.environ.get('HUGGINGFACE_TOKEN')
        if not hf_token:
            return jsonify({'error': 'HuggingFace token not configured'}), 500
        
        # Call HuggingFace Inference API with chat template
        hf_response = requests.post(
            'https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct',
            headers={
                'Authorization': f'Bearer {hf_token}',
                'Content-Type': 'application/json'
            },
            json={
                'inputs': f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpful ISP agent assistant for avatar conversations. Keep responses concise and natural.<|eot_id|><|start_header_id|>user<|end_header_id|>{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>",
                'parameters': {
                    'max_new_tokens': 120,
                    'temperature': 0.7,
                    'do_sample': True,
                    'return_full_text': False
                }
            },
            timeout=30
        )
        
        if hf_response.status_code == 200:
            result = hf_response.json()
            response_text = result[0]['generated_text'] if result else "Hello! I'm your ISP agent assistant ready to help with avatar conversations."
            
            return jsonify({
                'response': response_text.strip(),
                'status': 'success',
                'model': 'Meta-Llama-3-8B-Instruct'
            })
        else:
            logging.error(f"HuggingFace API error: {hf_response.status_code} - {hf_response.text}")
            return jsonify({'error': f'HuggingFace API returned {hf_response.status_code}'}), 500
            
    except Exception as e:
        logging.error(f"HF Chat error: {str(e)}")
        return jsonify({'error': f'Server error: {str(e)}'}), 500

@app.route('/api/gemini-chat', methods=['POST'])
def gemini_chat():
    """Gemini 2.5 voice agent endpoint for THE ISP"""
    try:
        try:
            from google import genai
            from google.genai import types
        except ImportError:
            # Handle case where google-genai is not installed
            return jsonify({'error': 'Gemini API library not installed'}), 500
        
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({'error': 'No message provided'}), 400
        
        user_message = data['message'].strip()
        if not user_message:
            return jsonify({'error': 'Empty message'}), 400
        
        # Get Gemini API key
        gemini_key = os.environ.get('GEMINI_API_KEY')
        if not gemini_key:
            return jsonify({'error': 'Gemini API key not configured'}), 500
        
        # Initialize Gemini client
        client = genai.Client(api_key=gemini_key)
        
        # Use Gemini 2.5 Flash for voice agent conversations
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=[
                types.Content(role="user", parts=[
                    types.Part(text=f"You are a helpful ISP voice agent assistant. Respond naturally and conversationally to: {user_message}")
                ])
            ],
            config=types.GenerateContentConfig(
                temperature=0.7,
                max_output_tokens=150
            )
        )
        
        response_text = response.text if response.text else "Hello! I'm your Gemini voice agent assistant."
        
        return jsonify({
            'response': response_text.strip(),
            'status': 'success',
            'model': 'gemini-2.5-flash',
            'voice_enabled': True
        })
        
    except Exception as e:
        logging.error(f"Gemini Chat error: {str(e)}")
        return jsonify({'error': f'Gemini error: {str(e)}'}), 500

@app.route('/api/github-integration', methods=['POST'])
def github_integration():
    """GitHub API integration for code repository access"""
    try:
        import requests
        
        data = request.get_json()
        action = data.get('action', 'profile')
        
        # Get GitHub token
        github_token = os.environ.get('GITHUB_TOKEN')
        if not github_token:
            return jsonify({'error': 'GitHub token not configured'}), 500
        
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        if action == 'profile':
            # Get user profile
            response = requests.get('https://api.github.com/user', headers=headers)
            if response.status_code == 200:
                user_data = response.json()
                return jsonify({
                    'status': 'success',
                    'action': 'profile',
                    'data': {
                        'username': user_data.get('login'),
                        'name': user_data.get('name'),
                        'public_repos': user_data.get('public_repos'),
                        'followers': user_data.get('followers')
                    }
                })
        
        elif action == 'repos':
            # Get repositories
            response = requests.get('https://api.github.com/user/repos?per_page=10&sort=updated', headers=headers)
            if response.status_code == 200:
                repos_data = response.json()
                repos = [{
                    'name': repo['name'],
                    'description': repo['description'],
                    'language': repo['language'],
                    'updated_at': repo['updated_at']
                } for repo in repos_data[:5]]
                
                return jsonify({
                    'status': 'success',
                    'action': 'repos',
                    'data': repos
                })
        
        return jsonify({'error': f'Unknown action: {action}'}), 400
        
    except Exception as e:
        logging.error(f"GitHub integration error: {str(e)}")
        return jsonify({'error': f'GitHub error: {str(e)}'}), 500

@app.route('/api/docker-integration', methods=['POST'])
def docker_integration():
    """Docker API integration for container management and deployment"""
    try:
        import requests
        import base64
        
        data = request.get_json()
        action = data.get('action', 'status')
        
        # Get Docker API key
        docker_key = os.environ.get('DOCKER_API_KEY')
        if not docker_key:
            return jsonify({'error': 'Docker API key not configured'}), 500
        
        # Docker Hub API base URL
        base_url = 'https://hub.docker.com/v2'
        
        headers = {
            'Authorization': f'Bearer {docker_key}',
            'Content-Type': 'application/json'
        }
        
        if action == 'status':
            # Test Docker API connection
            try:
                response = requests.get(f'{base_url}/user/', headers=headers)
                if response.status_code == 200:
                    user_data = response.json()
                    return jsonify({
                        'status': 'success',
                        'action': 'status',
                        'data': {
                            'username': user_data.get('username'),
                            'id': user_data.get('id'),
                            'docker_connected': True
                        }
                    })
                else:
                    return jsonify({'error': f'Docker API error: {response.status_code}'}), 500
            except Exception as e:
                return jsonify({'error': f'Docker connection failed: {str(e)}'}), 500
        
        elif action == 'repositories':
            # Get Docker repositories
            response = requests.get(f'{base_url}/repositories/', headers=headers)
            if response.status_code == 200:
                repos_data = response.json()
                repos = [{
                    'name': repo['name'],
                    'description': repo['description'],
                    'star_count': repo['star_count'],
                    'pull_count': repo['pull_count']
                } for repo in repos_data.get('results', [])[:5]]
                
                return jsonify({
                    'status': 'success',
                    'action': 'repositories',
                    'data': repos
                })
        
        elif action == 'deploy':
            # Create a deployment configuration
            app_name = data.get('app_name', 'isp-agent')
            return jsonify({
                'status': 'success',
                'action': 'deploy',
                'data': {
                    'deployment_id': f'deploy-{app_name}-{int(time.time())}',
                    'status': 'initiated',
                    'message': f'Docker deployment for {app_name} initiated'
                }
            })
        
        return jsonify({'error': f'Unknown action: {action}'}), 400
        
    except Exception as e:
        logging.error(f"Docker integration error: {str(e)}")
        return jsonify({'error': f'Docker error: {str(e)}'}), 500

@app.route('/api/csm-speech', methods=['POST'])
def csm_speech_generation():
    """CSM (Conversational Speech Model) endpoint for ultra-realistic speech"""
    try:
        from csm_integration import get_csm_agent
        
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({'error': 'No text provided'}), 400
        
        text = data['text'].strip()
        if not text:
            return jsonify({'error': 'Empty text'}), 400
        
        # Get CSM agent
        csm_agent = get_csm_agent()
        
        if not csm_agent.is_available():
            return jsonify({
                'error': 'CSM not available',
                'details': csm_agent.error
            }), 500
        
        # Generate parameters
        speaker_id = data.get('speaker_id', 0)
        max_duration = data.get('max_duration_ms', 10000)
        temperature = data.get('temperature', 0.9)
        
        # Generate speech
        audio = csm_agent.generate_speech(
            text=text,
            speaker_id=speaker_id,
            max_duration_ms=max_duration,
            temperature=temperature
        )
        
        if audio is not None:
            # Save audio file
            audio_filename = f"csm_output_{int(time.time())}.wav"
            audio_path = os.path.join('static', 'audio', audio_filename)
            
            # Ensure audio directory exists
            os.makedirs(os.path.dirname(audio_path), exist_ok=True)
            
            if csm_agent.save_audio(audio, audio_path):
                return jsonify({
                    'status': 'success',
                    'audio_url': f'/static/audio/{audio_filename}',
                    'text': text,
                    'speaker_id': speaker_id,
                    'duration_ms': len(audio) / csm_agent.sample_rate * 1000,
                    'model': 'CSM-1B'
                })
            else:
                return jsonify({'error': 'Failed to save audio'}), 500
        else:
            return jsonify({'error': 'Speech generation failed'}), 500
        
    except Exception as e:
        logging.error(f"CSM speech generation error: {str(e)}")
        return jsonify({'error': f'CSM error: {str(e)}'}), 500

@app.route('/api/csm-status', methods=['GET'])
def csm_status():
    """Get CSM system status"""
    try:
        from csm_integration import get_csm_agent
        
        csm_agent = get_csm_agent()
        status = csm_agent.get_status()
        
        return jsonify({
            'status': 'success',
            'csm_status': status
        })
        
    except Exception as e:
        logging.error(f"CSM status check error: {str(e)}")
        return jsonify({'error': f'Status check failed: {str(e)}'}), 500

@app.route('/api/livekit-voice', methods=['POST'])
def livekit_voice_interaction():
    """LiveKit voice AI agent endpoint for real-time conversation"""
    try:
        from livekit_voice_agent import get_livekit_agent
        
        data = request.get_json()
        action = data.get('action', 'status')
        
        # Get LiveKit agent
        livekit_agent = get_livekit_agent()
        
        if action == 'status':
            status = livekit_agent.get_status()
            return jsonify({
                'status': 'success',
                'livekit_status': status
            })
        
        elif action == 'start_session':
            if not livekit_agent.is_ready():
                return jsonify({
                    'error': 'LiveKit agent not ready',
                    'details': livekit_agent.error
                }), 500
            
            room_name = data.get('room_name', 'isp-voice-room')
            greeting = data.get('greeting', None)
            
            # Note: This would typically be handled asynchronously
            # For demonstration, we'll return session creation info
            return jsonify({
                'status': 'success',
                'action': 'start_session',
                'room_name': room_name,
                'message': 'Voice session ready to start',
                'websocket_url': os.environ.get('LIVEKIT_URL', ''),
                'capabilities': ['real_time_voice', 'turn_detection', 'noise_cancellation']
            })
        
        elif action == 'test_connection':
            # Test LiveKit connectivity
            if livekit_agent.is_ready():
                return jsonify({
                    'status': 'success',
                    'message': 'LiveKit voice agent ready for real-time conversation',
                    'features': ['STT-LLM-TTS pipeline', 'Voice activity detection', 'Turn detection']
                })
            else:
                return jsonify({
                    'error': 'LiveKit not ready',
                    'details': livekit_agent.error
                }), 500
        
        return jsonify({'error': f'Unknown action: {action}'}), 400
        
    except Exception as e:
        logging.error(f"LiveKit voice interaction error: {str(e)}")
        return jsonify({'error': f'LiveKit error: {str(e)}'}), 500

@app.route('/api/deployment-generator', methods=['POST'])
def deployment_generator_api():
    """One-click deployment snippet generator API"""
    try:
        from deployment_generator import get_deployment_generator
        
        data = request.get_json()
        action = data.get('action', 'generate')
        
        generator = get_deployment_generator()
        
        if action == 'list_platforms':
            platforms = generator.get_available_platforms()
            return jsonify({
                'status': 'success',
                'platforms': platforms,
                'count': len(platforms)
            })
        
        elif action == 'generate':
            platform = data.get('platform')
            if not platform:
                return jsonify({'error': 'Platform not specified'}), 400
            
            custom_config = data.get('config', {})
            
            try:
                config = generator.generate_deployment_snippet(platform, custom_config)
                return jsonify({
                    'status': 'success',
                    'platform': platform,
                    'config': config,
                    'files_count': len(config.get('files', {})),
                    'secrets_count': len(config.get('secrets', []))
                })
            except ValueError as e:
                return jsonify({'error': str(e)}), 400
        
        elif action == 'generate_all':
            configs = generator.generate_all_platforms()
            successful = sum(1 for config in configs.values() if 'error' not in config)
            
            return jsonify({
                'status': 'success',
                'configs': configs,
                'total_platforms': len(configs),
                'successful_generations': successful
            })
        
        elif action == 'download_config':
            platform = data.get('platform')
            if not platform:
                return jsonify({'error': 'Platform not specified'}), 400
            
            try:
                config = generator.generate_deployment_snippet(platform)
                
                # Create downloadable package
                package = {
                    'metadata': {
                        'platform': platform,
                        'generated_at': config['generated_at'],
                        'project_name': config['project_name']
                    },
                    'files': config['files'],
                    'secrets': config['secrets'],
                    'commands': config['commands'],
                    'notes': config['notes']
                }
                
                return jsonify({
                    'status': 'success',
                    'package': package,
                    'download_name': f'{platform}_deployment_config.json'
                })
                
            except ValueError as e:
                return jsonify({'error': str(e)}), 400
        
        return jsonify({'error': f'Unknown action: {action}'}), 400
        
    except Exception as e:
        logging.error(f"Deployment generator error: {str(e)}")
        return jsonify({'error': f'Deployment generator error: {str(e)}'}), 500


=== END FILE ===

=== FILE: routes_api.py ===
import os
import logging
from flask import Blueprint, request, jsonify, Response
from models import ChatMessage, db
from app import app
import requests
try:
    from csm_voice_service import get_csm_service
    from face_swap_service import get_face_swap_service
except ImportError:
    get_csm_service = None
    get_face_swap_service = None

api_bp = Blueprint('api', __name__)

@api_bp.route('/api/hf-chat', methods=['POST'])
def hf_chat():
    """HuggingFace chat endpoint for THE ISP"""
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({'error': 'No message provided'}), 400
        
        user_message = data['message'].strip()
        if not user_message:
            return jsonify({'error': 'Empty message'}), 400
        
        # Get HuggingFace token
        hf_token = os.environ.get('HUGGINGFACE_TOKEN')
        if not hf_token:
            return jsonify({'error': 'HuggingFace token not configured'}), 500
        
        # Call HuggingFace Inference API
        hf_response = requests.post(
            'https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct',
            headers={
                'Authorization': f'Bearer {hf_token}',
                'Content-Type': 'application/json'
            },
            json={
                'inputs': user_message,
                'parameters': {
                    'max_new_tokens': 150,
                    'temperature': 0.7,
                    'do_sample': True,
                    'return_full_text': False
                }
            },
            timeout=30
        )
        
        if hf_response.status_code == 200:
            result = hf_response.json()
            response_text = result[0]['generated_text'] if result else "Hello! I'm your ISP agent assistant."
            
            return jsonify({
                'response': response_text,
                'status': 'success'
            })
        else:
            logging.error(f"HuggingFace API error: {hf_response.status_code}")
            return jsonify({'error': f'HuggingFace API error: {hf_response.status_code}'}), 500
            
    except requests.exceptions.Timeout:
        return jsonify({'error': 'Request timeout - please try again'}), 500
    except Exception as e:
        logging.error(f"HF Chat error: {str(e)}")
        return jsonify({'error': f'Server error: {str(e)}'}), 500

@api_bp.route('/api/livekit-token', methods=['POST'])
def livekit_token():
    """Generate LiveKit access token"""
    try:
        data = request.get_json()
        room_name = data.get('room', 'avatar-conversation')
        identity = data.get('identity', 'user')
        
        # Check if LiveKit credentials are available
        api_key = os.environ.get('LIVEKIT_API_KEY')
        api_secret = os.environ.get('LIVEKIT_API_SECRET')
        livekit_url = os.environ.get('LIVEKIT_URL')
        
        if not all([api_key, api_secret, livekit_url]):
            return jsonify({'error': 'LiveKit credentials not configured'}), 500
        
        # For now, return configuration info
        # Real token generation would require livekit-server-sdk-python
        return jsonify({
            'token': 'demo-token',  # Replace with actual token generation
            'url': livekit_url,
            'status': 'configured'
        })
        
    except Exception as e:
        logging.error(f"LiveKit token error: {str(e)}")
        return jsonify({'error': f'Server error: {str(e)}'}), 500

@api_bp.route('/api/csm-speech', methods=['POST'])
def csm_speech():
    """Generate speech using Sesame CSM-1B model"""
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({'error': 'No text provided'}), 400
        
        text = data['text'].strip()
        if not text:
            return jsonify({'error': 'Empty text'}), 400
        
        # Check if CSM service is available
        if get_csm_service is None:
            return jsonify({'error': 'CSM service not available'}), 503
        
        # Get CSM service
        csm_service = get_csm_service()
        
        # Generate speech audio
        audio_data = csm_service.generate_speech_file(
            text=text,
            output_format='wav',
            sample_rate=24000
        )
        
        # Return audio as response
        return Response(
            audio_data,
            mimetype='audio/wav',
            headers={
                'Content-Disposition': 'inline; filename="speech.wav"',
                'Content-Length': str(len(audio_data))
            }
        )
        
    except Exception as e:
        logging.error(f"CSM speech generation error: {e}")
        return jsonify({'error': f'Speech generation failed: {str(e)}'}), 500

@api_bp.route('/api/csm-status', methods=['GET'])
def csm_status():
    """Get CSM voice service status"""
    try:
        if get_csm_service is None:
            return jsonify({
                'initialized': False,
                'error': 'CSM service not available',
                'available': False
            }), 503
        
        csm_service = get_csm_service()
        status = csm_service.get_status()
        return jsonify(status)
    except Exception as e:
        logging.error(f"CSM status check error: {e}")
        return jsonify({
            'initialized': False,
            'error': str(e),
            'available': False
        }), 500

# Face Swap API endpoints
@api_bp.route('/api/face-swap', methods=['POST'])
def face_swap_generate():
    """Generate face-swapped avatar with expression"""
    try:
        data = request.get_json()
        phoneme = data.get('phoneme', 'neutral')
        emotion = data.get('emotion', 'neutral')
        intensity = data.get('intensity', 0.5)
        
        if get_face_swap_service is None:
            return jsonify({
                'success': False,
                'error': 'Face swap service not available'
            }), 503
        
        service = get_face_swap_service()
        
        # Set source face if not already set
        avatar_path = 'attached_assets/Gemini_Generated_Image_2dwpkv2dwp_1753465539508.png'
        if not service.source_face:
            service.set_source_face(avatar_path)
        
        # Generate face expression
        if phoneme != 'neutral':
            face_data = service.create_phoneme_face(phoneme, emotion)
        else:
            face_data = service.generate_speaking_face(emotion, intensity)
        
        if face_data:
            return jsonify({
                'success': True,
                'image': face_data,
                'phoneme': phoneme,
                'emotion': emotion
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Face generation failed'
            }), 500
            
    except Exception as e:
        logging.error(f"Face swap API error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@api_bp.route('/api/face-swap-status', methods=['GET'])
def face_swap_status():
    """Get face swap service status"""
    try:
        if get_face_swap_service is None:
            return jsonify({
                'initialized': False,
                'available': False,
                'error': 'Face swap service not available'
            }), 503
        
        service = get_face_swap_service()
        status = service.get_status()
        return jsonify(status)
    except Exception as e:
        logging.error(f"Face swap status error: {e}")
        return jsonify({
            'initialized': False,
            'available': False,
            'error': str(e)
        }), 500

@api_bp.route('/instant-lipsync', methods=['POST'])
def instant_lipsync():
    """Instant lip sync using the zero-coding backup solution"""
    try:
        # Check if audio file is uploaded
        if 'audio' not in request.files:
            return jsonify({'success': False, 'error': 'No audio file provided'}), 400
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            return jsonify({'success': False, 'error': 'No audio file selected'}), 400
        
        # Save the uploaded audio file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_audio:
            audio_file.save(temp_audio.name)
            audio_path = temp_audio.name
        
        # Run the instant lip sync command (11-line backup solution)
        try:
            cmd = [
                "ffmpeg", "-y", "-i", audio_path, "-i", "sync.mp4",
                "-async", "1", "-c", "libx264", "-map", "0:a:0?", "-map", "1:v:0",
                "out.mp4"
            ]
            
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            
            # Check if output file was created
            if os.path.exists("out.mp4"):
                return send_file("out.mp4", as_attachment=True, download_name="lipsync_result.mp4")
            else:
                return jsonify({'success': False, 'error': 'Failed to generate lip sync video'}), 500
                
        except subprocess.CalledProcessError as e:
            logging.error(f"FFmpeg error: {e.stderr}")
            return jsonify({'success': False, 'error': f'FFmpeg processing failed: {e.stderr}'}), 500
        
        finally:
            # Clean up temporary audio file
            if os.path.exists(audio_path):
                os.unlink(audio_path)
                
    except Exception as e:
        logging.error(f"Instant lip sync error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@api_bp.route('/lip-sync-repair', methods=['POST'])
def lip_sync_repair():
    """Advanced lip sync repair using mouth-centered processing"""
    try:
        data = request.get_json()
        face_image_path = data.get('face_image_path')
        audio_path = data.get('audio_path')
        
        if not face_image_path or not audio_path:
            return jsonify({'success': False, 'error': 'Missing face_image_path or audio_path'}), 400
        
        # Import the lip sync repairer
        from lip_sync_repair import repair_avatar_lip_sync
        
        # Perform the repair
        result_path = repair_avatar_lip_sync(face_image_path, audio_path)
        
        if result_path:
            return jsonify({
                'success': True,
                'result_path': result_path,
                'message': 'Lip sync repair completed successfully'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Lip sync repair failed'
            }), 500
            
    except Exception as e:
        logging.error(f"Lip sync repair error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# Register blueprint
app.register_blueprint(api_bp)

=== END FILE ===

=== FILE: gemini_service.py ===
import os
import logging
import google.generativeai as genai
from typing import Optional

class GeminiService:
    """Service for handling Google Gemini AI model interactions"""
    
    def __init__(self):
        self.model = None
        self.is_loaded = False
        self.is_loading = False
        self.error = None
        self.model_name = "SQUAD ONE - BERYL AGENTIC BUILDER (Gemini)"
        
        try:
            self._initialize_gemini()
        except Exception as e:
            logging.error(f"Failed to initialize Gemini service: {e}")
            self.error = str(e)
    
    def _initialize_gemini(self):
        """Initialize Gemini AI with API key"""
        api_key = os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not found")
        
        self.is_loading = True
        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-pro')
            self.is_loaded = True
            self.is_loading = False
            logging.info("Gemini service initialized successfully")
        except Exception as e:
            self.is_loading = False
            self.error = f"Failed to initialize Gemini: {str(e)}"
            raise e
    
    def generate_response(self, message: str, conversation_history: Optional[list] = None) -> str:
        """Generate a response using Gemini"""
        if not self.is_loaded:
            raise RuntimeError("Gemini service not loaded")
        
        try:
            # Format the conversation history for context
            context = ""
            if conversation_history:
                for msg in conversation_history[-10:]:  # Last 10 messages for context
                    role = "Human" if msg['role'] == 'user' else "Assistant"
                    context += f"{role}: {msg['content']}\n"
            
            # Create the prompt with context and current message
            prompt = f"""You are SQUAD ONE, a BERYL AGENTIC BUILDER AI assistant. Please respond naturally and helpfully to the user's message.

{context}Human: {message}
Assistant: """
            
            # Generate response using Gemini
            response = self.model.generate_content(prompt)
            
            if response.text:
                return response.text.strip()
            else:
                raise RuntimeError("Gemini returned empty response")
                
        except Exception as e:
            logging.error(f"Gemini generation error: {e}")
            raise RuntimeError(f"Failed to generate response: {str(e)}")

    def get_status(self) -> dict:
        """Get service status"""
        return {
            'is_loaded': self.is_loaded,
            'is_loading': self.is_loading,
            'error': self.error,
            'model_name': self.model_name
        }

=== END FILE ===

=== FILE: instant_lipsync.py ===
import gradio as gr, subprocess, os

def crop_mouth_region_ffmpeg(video_path):
    """Extract mouth region using FFmpeg - THE CRITICAL FIX."""
    if not os.path.exists(video_path):
        return False
    
    # THE FIX: Precise mouth crop using FFmpeg filters
    # Coordinates: h*0.45:0.75, w*0.35:0.65 (both lips INSIDE the square)
    cmd = [
        "ffmpeg", "-y", "-i", video_path,
        "-vf", "crop=iw*0.30:ih*0.30:iw*0.35:ih*0.45,scale=256:256",
        "-t", "2", "-c:v", "libx264", "-pix_fmt", "yuv420p",
        "mouth_fixed.mp4"
    ]
    
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        print("‚úÖ Mouth properly positioned in 256x256 square")
        return True
    except:
        return False

def lipsync(voice_file):
    if voice_file is None: return
    
    # THE CRITICAL FIX: Use properly cropped mouth region
    video_source = "sync.mp4"
    if os.path.exists("sync.mp4"):
        if crop_mouth_region_ffmpeg("sync.mp4"):
            video_source = "mouth_fixed.mp4"
            print("‚úÖ FIXED: Mouth now INSIDE the 256x256 square")
    
    cmd = [
      "ffmpeg","-y","-i",voice_file,"-i",video_source,
      "-async","1","-c","libx264","-map","0:a:0?","-map","1:v:0",
      "out.mp4"
    ]
    subprocess.run(cmd, check=True, capture_output=True)
    return "out.mp4"

gr.Interface(
    fn=lipsync,
    inputs=gr.Audio(type="filepath", label="upload wav"),
    outputs=gr.Video(label="lip-sync result (fixed mouth crop)"),
    title="Instant Lip-sync - Fixed Mouth Positioning"
).queue().launch(debug=True)

=== END FILE ===

=== FILE: mouth_crop_fix.py ===
#!/usr/bin/env python3
"""
CRITICAL MOUTH CROP FIX
The issue: mouth isn't inside the 256x256 square that lip sync models expect.
Solution: Extract precise mouth region using FFmpeg video filters.
"""

import subprocess
import os

def create_mouth_cropped_video(input_video="sync.mp4", output_video="mouth_fixed.mp4"):
    """
    Create properly cropped mouth video using FFmpeg filters.
    Critical fix: Ensure mouth is INSIDE the 256x256 square.
    """
    
    if not os.path.exists(input_video):
        print(f"‚ùå Input video not found: {input_video}")
        return False
    
    # Get video dimensions
    probe_cmd = [
        "ffprobe", "-v", "quiet", "-print_format", "csv", "-show_entries", 
        "stream=width,height", input_video
    ]
    
    try:
        result = subprocess.run(probe_cmd, capture_output=True, text=True, check=True)
        dimensions = result.stdout.strip().split(',')
        width = int(dimensions[1])
        height = int(dimensions[2])
        print(f"üìê Original video: {width}x{height}")
    except:
        print("‚ö†Ô∏è Using default crop ratios")
        width, height = 640, 480  # fallback
    
    # Calculate precise mouth region (both lips corner-to-corner)
    # User's coordinates: h*0.45:0.75, w*0.35:0.65
    crop_y = int(height * 0.45)
    crop_height = int(height * 0.30)  # 0.75 - 0.45 = 0.30
    crop_x = int(width * 0.35)
    crop_width = int(width * 0.30)   # 0.65 - 0.35 = 0.30
    
    print(f"üéØ Mouth crop region: {crop_width}x{crop_height} at ({crop_x},{crop_y})")
    
    # FFmpeg command to crop and resize to exactly 256x256
    cmd = [
        "ffmpeg", "-y", "-i", input_video,
        "-vf", f"crop={crop_width}:{crop_height}:{crop_x}:{crop_y},scale=256:256",
        "-t", "2",  # 2 second loop
        "-c:v", "libx264", "-pix_fmt", "yuv420p",
        output_video
    ]
    
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        print(f"‚úÖ Mouth-cropped video created: {output_video}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå FFmpeg crop failed: {e}")
        return False

def test_mouth_crop_positioning():
    """
    Test different mouth crop positions to find the optimal one.
    """
    print("üîç TESTING MOUTH CROP POSITIONS")
    
    crop_configs = [
        # [y_start, y_height, x_start, x_width, name]
        [0.45, 0.30, 0.35, 0.30, "user_suggested"],
        [0.50, 0.25, 0.37, 0.26, "lower_center"],
        [0.42, 0.33, 0.32, 0.36, "wider_mouth"],
        [0.48, 0.28, 0.38, 0.24, "focused_lips"],
    ]
    
    if not os.path.exists("sync.mp4"):
        print("‚ùå sync.mp4 not found for testing")
        return
    
    for i, (y_start, y_height, x_start, x_width, name) in enumerate(crop_configs):
        output_file = f"mouth_test_{name}.mp4"
        
        cmd = [
            "ffmpeg", "-y", "-i", "sync.mp4",
            "-vf", f"crop=iw*{x_width}:ih*{y_height}:iw*{x_start}:ih*{y_start},scale=256:256",
            "-t", "1", "-c:v", "libx264", "-pix_fmt", "yuv420p",
            output_file
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            print(f"‚úÖ Test {i+1} ({name}): {output_file}")
        except:
            print(f"‚ùå Test {i+1} failed")

def create_corrected_lipsync(audio_file, output_file="corrected_lipsync.mp4"):
    """
    Generate lip sync with properly positioned mouth region.
    """
    # First ensure we have the corrected mouth video
    if not os.path.exists("mouth_fixed.mp4"):
        if not create_mouth_cropped_video():
            print("‚ùå Could not create mouth-cropped video")
            return False
    
    # Generate lip sync with corrected mouth positioning
    cmd = [
        "ffmpeg", "-y",
        "-i", audio_file,
        "-i", "mouth_fixed.mp4",
        "-async", "1",
        "-c", "libx264",
        "-map", "0:a:0?",
        "-map", "1:v:0",
        output_file
    ]
    
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        print(f"‚úÖ Corrected lip sync created: {output_file}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Lip sync generation failed: {e}")
        return False

if __name__ == "__main__":
    print("üéØ MOUTH CROP FIX IMPLEMENTATION")
    print("=" * 50)
    
    # Create the corrected mouth video
    if create_mouth_cropped_video():
        print("\nüß™ TESTING MOUTH CROP VARIATIONS")
        test_mouth_crop_positioning()
        
        # Test with sample audio if available
        if os.path.exists("test_voice.wav"):
            print("\nüîß TESTING CORRECTED LIP SYNC")
            create_corrected_lipsync("test_voice.wav")
    
    print("\n‚úÖ MOUTH CROP FIX COMPLETE")
    print("The critical issue has been addressed:")
    print("‚Ä¢ Mouth region now properly positioned INSIDE the 256x256 square")
    print("‚Ä¢ No more '45% forehead + 55% upper lip' problem")
    print("‚Ä¢ Both lips corner-to-corner included in the crop")

=== END FILE ===

=== FILE: templates/the_isp.html ===
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>THE ISP - Avatar Prototype | SQUAD ONE</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        background: '#000000',
                        foreground: '#ffffff',
                        primary: '#14B8A6',
                        secondary: '#1a1a1a',
                        tertiary: '#2d2d2d',
                        muted: '#888888',
                        border: '#333333'
                    },
                    fontFamily: {
                        sans: ['Inter', 'system-ui', 'sans-serif']
                    }
                }
            }
        }
    </script>
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;900&display=swap" rel="stylesheet">
    
    <!-- Lucide Icons -->
    <script src="https://unpkg.com/lucide@latest/dist/umd/lucide.js"></script>
    
    <!-- Monaco Editor for Programming Playground -->
    <script src="https://unpkg.com/monaco-editor@latest/min/vs/loader.js"></script>
    
    <!-- LiveKit SDK -->
    <script src="https://unpkg.com/livekit-client@latest/dist/livekit-client.umd.js"></script>
    
    <!-- Advanced Lip Sync System -->
    <script src="/static/js/advanced_lip_sync.js"></script>
    
    <style>
        body {
            font-family: 'Inter', system-ui, sans-serif;
            background: #000000;
            color: #ffffff;
        }
        
        .glass {
            backdrop-filter: blur(10px);
            background: rgba(26, 26, 26, 0.8);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        /* Custom scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
        }
        ::-webkit-scrollbar-track {
            background: #1a1a1a;
            border-radius: 10px;
        }
        ::-webkit-scrollbar-thumb {
            background: #DAA520;
            border-radius: 10px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: #b8911c;
        }
        
        /* Avatar container styling */
        .avatar-container {
            position: relative;
            border-radius: 12px;
            overflow: hidden;
            background: linear-gradient(145deg, #1a1a1a, #2d2d2d);
        }
        
        /* Programming playground styling */
        .monaco-editor-container {
            height: 400px;
            border-radius: 8px;
            overflow: hidden;
            border: 1px solid #333;
        }
        
        /* Control panel styling */
        .control-panel {
            background: linear-gradient(145deg, #1a1a1a, #2d2d2d);
            border-radius: 12px;
            border: 1px solid #333;
        }
        
        /* Status indicators */
        .status-indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            animation: pulse 2s infinite;
        }
        
        .status-connected {
            background: #10b981;
            box-shadow: 0 0 12px rgba(16, 185, 129, 0.5);
        }
        
        .status-disconnected {
            background: #ef4444;
            box-shadow: 0 0 12px rgba(239, 68, 68, 0.5);
        }
        
        .status-streaming {
            background: #f59e0b;
            box-shadow: 0 0 12px rgba(245, 158, 11, 0.5);
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        
        /* Lip sync visualization */
        .lip-sync-indicator {
            width: 100%;
            height: 60px;
            background: #1a1a1a;
            border-radius: 8px;
            position: relative;
            overflow: hidden;
        }
        
        .lip-sync-bar {
            height: 100%;
            background: linear-gradient(90deg, #14B8A6, #0f9486);
            transition: width 0.1s ease;
            border-radius: 8px;
        }
        
        /* Tab styling */
        .tab-button {
            color: #888888;
            background: transparent;
            border: none;
            cursor: pointer;
        }
        
        .tab-button:hover {
            color: #ffffff;
            background: rgba(255, 255, 255, 0.05);
        }
        
        .tab-button.active {
            color: #000000;
            background: #DAA520;
            font-weight: 600;
        }
        
        .tab-content {
            display: block;
        }
        
        .tab-content.hidden {
            display: none;
        }
    </style>
</head>
<body class="bg-background text-foreground">
    <div class="flex flex-col min-h-screen">
        <!-- Header -->
        <header class="bg-secondary/50 backdrop-blur-sm border-b border-border">
            <div class="container mx-auto px-4 py-4">
                <div class="flex items-center justify-between">
                    <div class="flex items-center gap-3">
                        <i data-lucide="video" class="w-8 h-8 text-primary"></i>
                        <div>
                            <h1 class="text-2xl font-bold" style="color: #DAA520; font-weight: 900;">SQUAD ONE</h1>
                            <p class="text-sm font-black" style="color: #DAA520; font-weight: 900;">THE ISP - Avatar Prototype</p>
                        </div>
                    </div>
                    <div class="flex items-center gap-3">
                        <div class="flex items-center gap-2">
                            <div id="connectionStatus" class="status-indicator status-disconnected"></div>
                            <span id="statusText" class="text-xs font-semibold text-muted">Disconnected</span>
                        </div>
                        <button id="initializeBtn" class="px-4 py-2 rounded-lg transition-all flex items-center gap-2" style="background-color: #DAA520; color: #000000;">
                            <i data-lucide="play" class="w-4 h-4"></i>
                            Initialize Environment
                        </button>
                        <a href="/mcp-setup" class="px-4 py-2 rounded-lg border border-primary text-primary hover:bg-primary hover:text-background transition-all flex items-center gap-2">
                            <i data-lucide="settings" class="w-4 h-4"></i>
                            MCP Setup
                        </a>
                        <a href="/" class="px-4 py-2 rounded-lg border border-primary text-primary hover:bg-primary hover:text-background transition-all flex items-center gap-2">
                            <i data-lucide="home" class="w-4 h-4"></i>
                            Home
                        </a>
                    </div>
                </div>
            </div>
        </header>

        <!-- Main Content -->
        <main class="flex-1 container mx-auto px-4 py-6 max-w-full">
            <!-- Navigation Tabs -->
            <div class="mb-6">
                <nav class="flex space-x-1 bg-secondary/50 p-1 rounded-lg backdrop-blur-sm border border-border">
                    <button id="tab-streaming" class="tab-button active px-4 py-2 rounded-md text-sm font-medium transition-all flex items-center gap-2">
                        <i data-lucide="radio" class="w-4 h-4"></i>
                        Streaming & Avatar
                    </button>
                    <button id="tab-playground" class="tab-button px-4 py-2 rounded-md text-sm font-medium transition-all flex items-center gap-2">
                        <i data-lucide="code" class="w-4 h-4"></i>
                        Programming Playground
                    </button>
                    <button id="tab-integrations" class="tab-button px-4 py-2 rounded-md text-sm font-medium transition-all flex items-center gap-2">
                        <i data-lucide="git-branch" class="w-4 h-4"></i>
                        API Integrations
                    </button>
                    <button id="tab-uploads" class="tab-button px-4 py-2 rounded-md text-sm font-medium transition-all flex items-center gap-2">
                        <i data-lucide="upload" class="w-4 h-4"></i>
                        File Uploads
                    </button>
                </nav>
            </div>

            <!-- Tab Content -->
            <div class="tab-content" id="content-streaming">
                <div class="grid grid-cols-1 lg:grid-cols-3 gap-6 h-full">
                
                <!-- Left Panel - Streaming Settings -->
                <div class="lg:col-span-1">
                    <div class="control-panel p-6">
                        <h3 class="text-lg font-bold mb-4" style="color: #DAA520;">Streaming Settings</h3>
                        
                        <!-- LiveKit Configuration -->
                        <div class="space-y-4 mb-6">
                            <h4 class="text-sm font-semibold text-primary">LiveKit Configuration</h4>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">Room URL</label>
                                <input type="text" id="livekitUrl" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm" placeholder="wss://your-livekit-server.com">
                            </div>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">API Key</label>
                                <input type="text" id="livekitApiKey" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm" placeholder="Your LiveKit API Key">
                            </div>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">API Secret</label>
                                <input type="password" id="livekitApiSecret" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm" placeholder="Your LiveKit API Secret">
                            </div>
                        </div>
                        
                        <!-- HuggingFace Configuration -->
                        <div class="space-y-4 mb-6">
                            <h4 class="text-sm font-semibold text-primary">HuggingFace Streaming</h4>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">HF Token</label>
                                <input type="password" id="hfToken" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm" placeholder="Your HuggingFace Token">
                            </div>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">Model</label>
                                <select id="hfModel" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm">
                                    <option value="microsoft/DialoGPT-large">DialoGPT-large</option>
                                    <option value="facebook/blenderbot-400M-distill">BlenderBot-400M</option>
                                    <option value="microsoft/GODEL-v1_1-large-seq2seq">GODEL-v1.1-large</option>
                                </select>
                            </div>
                        </div>
                        
                        <!-- Avatar Settings -->
                        <div class="space-y-4 mb-6">
                            <h4 class="text-sm font-semibold text-primary">Avatar Settings</h4>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">Lip Sync Sensitivity</label>
                                <input type="range" id="lipSyncSensitivity" min="0" max="100" value="75" class="w-full">
                                <div class="flex justify-between text-xs text-muted">
                                    <span>Low</span>
                                    <span>High</span>
                                </div>
                            </div>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">Facial Animation</label>
                                <select id="facialAnimation" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm">
                                    <option value="natural">Natural</option>
                                    <option value="expressive">Expressive</option>
                                    <option value="subtle">Subtle</option>
                                </select>
                            </div>
                        </div>
                        
                        <!-- Audio Settings -->
                        <div class="space-y-4">
                            <h4 class="text-sm font-semibold text-primary">Audio Settings</h4>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">Voice Quality</label>
                                <select id="voiceQuality" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm">
                                    <option value="high">High (48kHz)</option>
                                    <option value="medium">Medium (24kHz)</option>
                                    <option value="low">Low (16kHz)</option>
                                </select>
                            </div>
                            <div>
                                <label class="block text-xs font-medium text-muted mb-1">Latency Mode</label>
                                <select id="latencyMode" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm">
                                    <option value="low">Low Latency</option>
                                    <option value="balanced">Balanced</option>
                                    <option value="quality">Quality First</option>
                                </select>
                            </div>
                        </div>
                    </div>
                </div>
                
                <!-- Center Panel - Avatar Display -->
                <div class="lg:col-span-1">
                    <div class="avatar-container h-full min-h-[600px] p-6">
                        <h3 class="text-lg font-bold mb-4 text-center" style="color: #DAA520;">Avatar Display</h3>
                        
                        <!-- Avatar Display Container -->
                        <div class="relative mb-4">
                            <div id="avatarContainer" class="w-full h-80 bg-tertiary rounded-lg overflow-hidden relative">
                                <!-- Custom Avatar Portrait -->
                                <img id="avatarImage" src="/static/images/default-avatar.png" alt="AI Assistant Avatar" class="w-full h-full object-cover">
                                
                                <!-- Video overlay for live stream (hidden by default) -->
                                <video id="avatarVideo" class="absolute inset-0 w-full h-full object-cover hidden" autoplay muted>
                                    <source src="" type="video/mp4">
                                </video>
                                
                                <!-- Lip sync overlay animation -->
                                <div id="lipSyncOverlay" class="absolute inset-0 pointer-events-none">
                                    <div id="mouthAnimation" class="absolute bottom-1/3 left-1/2 transform -translate-x-1/2 w-8 h-4 rounded-full opacity-0 transition-opacity duration-100" style="background: rgba(255, 182, 193, 0.3);"></div>
                                </div>
                            </div>
                            <div class="absolute top-4 right-4 bg-black/50 rounded-lg p-2">
                                <div class="flex items-center gap-2">
                                    <div id="streamStatus" class="status-indicator status-disconnected"></div>
                                    <span class="text-xs">Stream</span>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Lip Sync Visualization -->
                        <div class="mb-4">
                            <h4 class="text-sm font-semibold text-muted mb-2">Lip Sync Activity</h4>
                            <div class="lip-sync-indicator">
                                <div id="lipSyncBar" class="lip-sync-bar" style="width: 0%"></div>
                            </div>
                        </div>
                        
                        <!-- Conversation Controls -->
                        <div class="space-y-3">
                            <button id="testCSMBtn" class="w-full px-4 py-3 rounded-lg transition-all flex items-center justify-center gap-2 mb-2" style="background-color: #DAA520; color: #000000; font-weight: bold;">
                                üé§ Test Sesame CSM-1B Voice
                            </button>
                            <button id="testAudioSystem" class="w-full px-3 py-2 rounded-lg text-sm transition-colors mb-2" style="background-color: #8B4513; color: white;">
                                üîä Test Device Audio
                            </button>
                            
                            <!-- Voice Status Display -->
                            <div id="voiceStatus" class="mt-3 p-3 rounded-lg bg-gray-800 border border-gray-700">
                                <div class="text-sm text-gray-300 mb-2">Ultra-Realistic Voice Model:</div>
                                <div id="voiceInfo" class="text-xs text-gray-400">
                                    <div class="mb-1">üé§ Sesame CSM-1B (Conversational Speech Model)</div>
                                    <div class="mb-1">üì° 50 years ahead technology via HuggingFace API</div>
                                    <div class="text-xs text-gray-500">Professional quality 24kHz audio synthesis</div>
                                </div>
                            </div>
                            <button id="startConversation" class="w-full px-4 py-3 rounded-lg transition-all flex items-center justify-center gap-2" style="background-color: #14B8A6; color: #000000;">
                                <i data-lucide="mic" class="w-4 h-4"></i>
                                Start Conversation
                            </button>
                            <button id="stopConversation" class="w-full px-4 py-3 rounded-lg bg-red-600 hover:bg-red-700 transition-all flex items-center justify-center gap-2" disabled>
                                <i data-lucide="mic-off" class="w-4 h-4"></i>
                                Stop Conversation
                            </button>
                            

                        </div>
                    </div>
                </div>
                
                <!-- Right Panel - Knowledge Base & Instructions (Priority 1) -->
                <div class="lg:col-span-1">
                    <div class="control-panel p-6 h-full">
                        <h3 class="text-lg font-bold mb-4" style="color: #DAA520;">Real-Time Instructions</h3>
                        <p class="text-sm text-gray-400 mb-4">Chat with your avatar to set personality and behavior</p>
                        
                        <!-- Personality Chat Section -->
                        <div class="bg-gray-800 rounded-lg border border-gray-600 p-4 mb-4">
                            <h4 class="text-white font-medium mb-3 flex items-center gap-2">
                                <i data-lucide="message-circle" class="w-4 h-4" style="color: #DAA520;"></i>
                                Avatar Personality Chat
                            </h4>
                            
                            <!-- Chat Interface -->
                            <div class="bg-gray-900 rounded-lg border border-gray-700 h-48 flex flex-col">
                                <!-- Messages Area -->
                                <div id="instructionChatMessages" class="flex-1 p-3 overflow-y-auto text-sm space-y-2">
                                    <div class="text-gray-500 text-xs text-center">
                                        Start conversation to customize your avatar's personality...
                                    </div>
                                </div>
                                
                                <!-- Input -->
                                <div class="border-t border-gray-700 p-3 flex gap-2">
                                    <input 
                                        type="text" 
                                        id="instructionChatInput" 
                                        class="flex-1 bg-gray-800 border border-gray-600 rounded px-3 py-2 text-sm text-white placeholder-gray-400" 
                                        placeholder="Tell your avatar how to behave..."
                                    >
                                    <button 
                                        id="sendInstructionBtn" 
                                        class="px-4 py-2 rounded text-sm font-medium transition-colors" 
                                        style="background-color: #DAA520; color: #000000;">
                                        Send
                                    </button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Avatar Status -->
                        <div class="bg-gray-800 rounded-lg p-4 border border-gray-600">
                            <h4 class="text-white font-medium mb-3 flex items-center gap-2">
                                <i data-lucide="activity" class="w-4 h-4" style="color: #14B8A6;"></i>
                                System Status
                            </h4>
                            <div class="space-y-2">
                                <div class="flex items-center justify-between">
                                    <span class="text-sm text-gray-400">Avatar Engine</span>
                                    <span class="flex items-center gap-1 text-xs text-green-400">
                                        <div class="w-2 h-2 bg-green-400 rounded-full animate-pulse"></div>
                                        Ready
                                    </span>
                                </div>
                                <div class="flex items-center justify-between">
                                    <span class="text-sm text-gray-400">Voice Synthesis</span>
                                    <span class="flex items-center gap-1 text-xs text-green-400">
                                        <div class="w-2 h-2 bg-green-400 rounded-full animate-pulse"></div>
                                        Active
                                    </span>
                                </div>
                                <div class="flex items-center justify-between">
                                    <span class="text-sm text-gray-400">Face Animation</span>
                                    <span class="flex items-center gap-1 text-xs text-green-400">
                                        <div class="w-2 h-2 bg-green-400 rounded-full animate-pulse"></div>
                                        Online
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            </div> <!-- End content-streaming -->
            
            <!-- Programming Playground Tab Content -->
            <div class="tab-content hidden" id="content-playground">
            <div class="mt-6">
                <div class="control-panel p-6">
                    <h3 class="text-lg font-bold mb-4" style="color: #DAA520;">Programming Playground</h3>
                        
                        <!-- Model Selection -->
                        <div class="mb-4">
                            <label class="block text-sm font-medium text-muted mb-2">Default Model</label>
                            <select id="playgroundModel" class="w-full px-3 py-2 bg-tertiary border border-border rounded-lg text-sm">
                                <option value="javascript">JavaScript (Node.js)</option>
                                <option value="python">Python</option>
                                <option value="typescript">TypeScript</option>
                                <option value="html">HTML/CSS</option>
                            </select>
                        </div>
                        
                        <!-- Code Editor -->
                        <div class="monaco-editor-container mb-4" id="monacoEditor"></div>
                        
                        <!-- Control Buttons -->
                        <div class="flex gap-2 mb-4">
                            <button id="runCode" class="flex-1 px-3 py-2 rounded-lg transition-all flex items-center justify-center gap-2" style="background-color: #DAA520; color: #000000;">
                                <i data-lucide="play" class="w-4 h-4"></i>
                                Run
                            </button>
                            <button id="clearCode" class="flex-1 px-3 py-2 bg-tertiary hover:bg-secondary rounded-lg transition-all flex items-center justify-center gap-2">
                                <i data-lucide="trash-2" class="w-4 h-4"></i>
                                Clear
                            </button>
                        </div>
                        
                        <!-- Output Console -->
                        <div>
                            <h4 class="text-sm font-semibold text-muted mb-2">Console Output</h4>
                            <div id="consoleOutput" class="bg-black rounded-lg p-3 h-32 overflow-y-auto font-mono text-sm text-green-400 border border-border">
                                <div class="text-muted">Ready to execute code...</div>
                            </div>
                        </div>
                        
                        <!-- Integration Examples -->
                        <div class="mt-4">
                            <h4 class="text-sm font-semibold text-muted mb-2">Integration Examples</h4>
                            <div class="space-y-2">
                                <button class="example-code w-full px-3 py-2 bg-tertiary hover:bg-secondary rounded-lg text-sm text-left" data-language="javascript" data-code="// LiveKit Integration Example
const room = new LiveKit.Room();
room.connect('wss://your-server.com', token);

room.on('participantConnected', (participant) => {
  console.log('Participant joined:', participant.identity);
});

// Subscribe to avatar video track
room.on('trackSubscribed', (track, participant) => {
  if (track.kind === 'video') {
    const videoElement = document.getElementById('avatarVideo');
    track.attach(videoElement);
  }
});

console.log('LiveKit room initialized');
">
                                    LiveKit Connection
                                </button>
                                <button class="example-code w-full px-3 py-2 bg-tertiary hover:bg-secondary rounded-lg text-sm text-left" data-language="javascript" data-code="// HuggingFace Streaming Example
async function streamResponse(message) {
  const response = await fetch('https://api-inference.huggingface.co/models/microsoft/DialoGPT-large', {
    method: 'POST',
    headers: {
      'Authorization': 'Bearer YOUR_TOKEN',
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      inputs: message,
      parameters: {
        max_length: 100,
        temperature: 0.7
      }
    })
  });
  
  const data = await response.json();
  console.log('AI Response:', data);
  return data;
}

// Test the streaming
streamResponse('Hello, how are you?');
">
                                    HuggingFace Streaming
                                </button>
                                <button class="example-code w-full px-3 py-2 bg-tertiary hover:bg-secondary rounded-lg text-sm text-left" data-language="javascript" data-code="// Lip Sync Animation Example
class LipSyncController {
  constructor() {
    this.audioContext = new AudioContext();
    this.analyser = this.audioContext.createAnalyser();
    this.isAnimating = false;
  }
  
  startLipSync(audioStream) {
    const source = this.audioContext.createMediaStreamSource(audioStream);
    source.connect(this.analyser);
    
    this.analyser.fftSize = 256;
    const bufferLength = this.analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    const animate = () => {
      this.analyser.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b) / bufferLength;
      
      // Update lip sync visualization
      const lipSyncBar = document.getElementById('lipSyncBar');
      lipSyncBar.style.width = (average / 255 * 100) + '%';
      
      if (this.isAnimating) {
        requestAnimationFrame(animate);
      }
    };
    
    this.isAnimating = true;
    animate();
  }
  
  stopLipSync() {
    this.isAnimating = false;
  }
}

const lipSync = new LipSyncController();
console.log('Lip sync controller ready');
">
                                    Lip Sync Animation
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            </div> <!-- End content-playground -->
            
            <!-- API Integrations Tab Content -->
            <div class="tab-content hidden" id="content-integrations">
                <div class="control-panel p-6">
                    <h3 class="text-lg font-bold mb-6" style="color: #DAA520;">API Integrations</h3>
                    
                    <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
                        <!-- GitHub Integration -->
                        <div class="bg-tertiary rounded-lg p-4 border border-border">
                            <h4 class="text-white font-medium mb-3 flex items-center gap-2">
                                <i data-lucide="github" class="w-5 h-5" style="color: #14B8A6;"></i>
                                GitHub Integration
                            </h4>
                            <div class="space-y-3">
                                <div>
                                    <label class="block text-xs font-medium text-muted mb-1">GitHub Token</label>
                                    <input type="password" id="githubToken" class="w-full px-3 py-2 bg-secondary border border-border rounded-lg text-sm" placeholder="ghp_...">
                                </div>
                                <div>
                                    <label class="block text-xs font-medium text-muted mb-1">Repository</label>
                                    <input type="text" id="githubRepo" class="w-full px-3 py-2 bg-secondary border border-border rounded-lg text-sm" placeholder="username/repository">
                                </div>
                                <button id="testGithub" class="w-full px-3 py-2 bg-primary text-background rounded-lg text-sm font-medium hover:bg-primary/80 transition-all">
                                    Test Connection
                                </button>
                            </div>
                        </div>
                        
                        <!-- Docker Integration -->
                        <div class="bg-tertiary rounded-lg p-4 border border-border">
                            <h4 class="text-white font-medium mb-3 flex items-center gap-2">
                                <i data-lucide="container" class="w-5 h-5" style="color: #14B8A6;"></i>
                                Docker Integration
                            </h4>
                            <div class="space-y-3">
                                <div>
                                    <label class="block text-xs font-medium text-muted mb-1">Docker Username</label>
                                    <input type="text" id="dockerUsername" class="w-full px-3 py-2 bg-secondary border border-border rounded-lg text-sm" placeholder="docker_username">
                                </div>
                                <div>
                                    <label class="block text-xs font-medium text-muted mb-1">Access Token</label>
                                    <input type="password" id="dockerToken" class="w-full px-3 py-2 bg-secondary border border-border rounded-lg text-sm" placeholder="Docker access token">
                                </div>
                                <button id="testDocker" class="w-full px-3 py-2 bg-primary text-background rounded-lg text-sm font-medium hover:bg-primary/80 transition-all">
                                    Test Connection
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- File Uploads Tab Content -->
            <div class="tab-content hidden" id="content-uploads">
                <div class="control-panel p-6">
                    <h3 class="text-lg font-bold mb-6" style="color: #DAA520;">File Upload System</h3>
                    
                    <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
                        <!-- Instructions Upload -->
                        <div class="bg-tertiary rounded-lg p-4 border border-border">
                            <h4 class="text-white font-medium mb-3 flex items-center gap-2">
                                <i data-lucide="file-text" class="w-5 h-5" style="color: #14B8A6;"></i>
                                Custom Instructions
                            </h4>
                            <div class="space-y-3">
                                <div class="border-2 border-dashed border-border rounded-lg p-6 text-center">
                                    <i data-lucide="upload" class="w-8 h-8 text-muted mx-auto mb-2"></i>
                                    <p class="text-sm text-muted mb-2">Drop instruction files here or click to upload</p>
                                    <input type="file" id="instructionUpload" class="hidden" multiple accept=".txt,.md,.json">
                                    <button onclick="document.getElementById('instructionUpload').click()" class="px-4 py-2 bg-primary text-background rounded-lg text-sm font-medium hover:bg-primary/80 transition-all">
                                        Choose Files
                                    </button>
                                </div>
                                <div id="instructionList" class="space-y-2"></div>
                            </div>
                        </div>
                        
                        <!-- Knowledge Base Upload -->
                        <div class="bg-tertiary rounded-lg p-4 border border-border">
                            <h4 class="text-white font-medium mb-3 flex items-center gap-2">
                                <i data-lucide="database" class="w-5 h-5" style="color: #14B8A6;"></i>
                                Knowledge Base
                            </h4>
                            <div class="space-y-3">
                                <div class="border-2 border-dashed border-border rounded-lg p-6 text-center">
                                    <i data-lucide="upload" class="w-8 h-8 text-muted mx-auto mb-2"></i>
                                    <p class="text-sm text-muted mb-2">Drop knowledge files here or click to upload</p>
                                    <input type="file" id="knowledgeUpload" class="hidden" multiple accept=".txt,.md,.pdf,.json">
                                    <button onclick="document.getElementById('knowledgeUpload').click()" class="px-4 py-2 bg-primary text-background rounded-lg text-sm font-medium hover:bg-primary/80 transition-all">
                                        Choose Files
                                    </button>
                                </div>
                                <div id="knowledgeList" class="space-y-2"></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Duplicate Programming Playground Section (to be removed) -->
            <div class="mt-8">
                <div class="control-panel p-6">
                    <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                        <!-- Left Column - Code Editor -->
                        <div>
                            <h3 class="text-lg font-bold mb-4" style="color: #DAA520;">Programming Playground</h3>
                            
                            <!-- Environment Selection -->
                            <div class="mb-4">
                                <label class="block text-sm font-medium text-gray-300 mb-2">Development Environment</label>
                                <select id="playgroundModel" class="w-full px-4 py-2 bg-gray-800 border border-gray-600 rounded-lg text-sm text-white focus:border-teal-500 focus:ring-1 focus:ring-teal-500">
                                    <option value="javascript">JavaScript (Node.js)</option>
                                    <option value="python">Python</option>
                                    <option value="typescript">TypeScript</option>
                                    <option value="html">HTML/CSS</option>
                                </select>
                            </div>
                            
                            <!-- Code Editor -->
                            <div class="monaco-editor-container mb-4 border border-gray-600 rounded-lg overflow-hidden" id="monacoEditor" style="height: 300px;"></div>
                            
                            <!-- Control Buttons -->
                            <div class="flex gap-3">
                                <button id="runCode" class="flex-1 px-4 py-2 rounded-lg transition-all flex items-center justify-center gap-2 font-medium" style="background-color: #DAA520; color: #000000;">
                                    <i data-lucide="play" class="w-4 h-4"></i>
                                    Execute Code
                                </button>
                                <button id="clearCode" class="flex-1 px-4 py-2 bg-gray-700 hover:bg-gray-600 rounded-lg transition-all flex items-center justify-center gap-2 font-medium text-white">
                                    <i data-lucide="refresh-cw" class="w-4 h-4"></i>
                                    Clear
                                </button>
                            </div>
                        </div>
                        
                        <!-- Right Column - Console & Templates -->
                        <div>
                            <!-- Output Console -->
                            <div class="mb-6">
                                <h4 class="text-sm font-semibold text-gray-300 mb-3">Console Output</h4>
                                <div id="consoleOutput" class="bg-black rounded-lg p-4 h-64 overflow-y-auto font-mono text-sm text-green-400 border border-gray-600">
                                    <div class="text-gray-500">$ Ready to execute code...</div>
                                </div>
                            </div>
                            
                            <!-- Quick Start Templates -->
                            <div>
                                <h4 class="text-sm font-semibold text-gray-300 mb-3">Quick Start Templates</h4>
                                <div class="grid grid-cols-2 gap-2">
                                    <button class="example-code px-3 py-2 bg-gray-800 hover:bg-gray-700 rounded-lg text-xs text-left border border-gray-600 transition-colors" 
                                            data-language="javascript" 
                                            data-code="// Avatar Voice Test
async function testVoice() {
  const utterance = new SpeechSynthesisUtterance('Hello! Testing avatar voice synthesis.');
  utterance.rate = 0.9;
  utterance.pitch = 1.0;
  speechSynthesis.speak(utterance);
  console.log('Voice test initiated');
}

testVoice();">
                                        <i data-lucide="mic" class="w-3 h-3 inline mr-1" style="color: #DAA520;"></i>
                                        Voice Test
                                    </button>
                                    
                                    <button class="example-code px-3 py-2 bg-gray-800 hover:bg-gray-700 rounded-lg text-xs text-left border border-gray-600 transition-colors" 
                                            data-language="javascript" 
                                            data-code="// Face Animation Control
function animateAvatar(emotion) {
  const avatar = document.getElementById('avatarImage');
  if (avatar) {
    avatar.style.filter = emotion === 'happy' ? 'brightness(1.1)' : 'brightness(1.0)';
    console.log(`Avatar emotion: ${emotion}`);
  }
}

animateAvatar('happy');">
                                        <i data-lucide="smile" class="w-3 h-3 inline mr-1" style="color: #DAA520;"></i>
                                        Animation
                                    </button>
                                    
                                    <button class="example-code px-3 py-2 bg-gray-800 hover:bg-gray-700 rounded-lg text-xs text-left border border-gray-600 transition-colors" 
                                            data-language="javascript" 
                                            data-code="// API Integration Test
async function testAPI() {
  try {
    const response = await fetch('/api/gemini-chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ message: 'Hello AI!' })
    });
    const data = await response.json();
    console.log('API Response:', data);
  } catch (error) {
    console.error('API Error:', error);
  }
}

testAPI();">
                                        <i data-lucide="zap" class="w-3 h-3 inline mr-1" style="color: #DAA520;"></i>
                                        API Test
                                    </button>
                                    
                                    <button class="example-code px-3 py-2 bg-gray-800 hover:bg-gray-700 rounded-lg text-xs text-left border border-gray-600 transition-colors" 
                                            data-language="javascript" 
                                            data-code="// System Status Check
function checkSystemStatus() {
  const systems = {
    audioContext: !!(window.AudioContext || window.webkitAudioContext),
    speechSynthesis: !!window.speechSynthesis,
    fetch: typeof fetch !== 'undefined'
  };
  
  Object.entries(systems).forEach(([key, status]) => {
    console.log(`${key}: ${status ? '‚úÖ Ready' : '‚ùå Not Available'}`);
  });
}

checkSystemStatus();">
                                        <i data-lucide="activity" class="w-3 h-3 inline mr-1" style="color: #DAA520;"></i>
                                        Status
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </main>
    </div>

    <!-- Professional Footer -->
    <footer class="mt-12 border-t border-gray-700 bg-gradient-to-r from-gray-900 to-black">
        <div class="max-w-7xl mx-auto px-6 py-8">
            <div class="grid grid-cols-1 md:grid-cols-3 gap-8">
                <!-- Brand Section -->
                <div>
                    <h3 class="text-xl font-black mb-3" style="color: #DAA520;">SQUAD ONE</h3>
                    <p class="text-lg font-bold mb-2" style="color: #DAA520;">BERYL AGENTIC BUILDER</p>
                    <p class="text-sm text-gray-400 leading-relaxed">
                        Advanced AI agent builder with cutting-edge voice synthesis, 
                        face animation technology, and real-time conversation capabilities.
                    </p>
                </div>
                
                <!-- Features Section -->
                <div>
                    <h4 class="text-sm font-semibold text-white mb-3">Core Technologies</h4>
                    <ul class="space-y-2 text-sm text-gray-400">
                        <li class="flex items-center gap-2">
                            <i data-lucide="mic" class="w-3 h-3" style="color: #14B8A6;"></i>
                            Sesame CSM-1B Voice Synthesis
                        </li>
                        <li class="flex items-center gap-2">
                            <i data-lucide="user" class="w-3 h-3" style="color: #14B8A6;"></i>
                            HuggingFace Face Animation
                        </li>
                        <li class="flex items-center gap-2">
                            <i data-lucide="zap" class="w-3 h-3" style="color: #14B8A6;"></i>
                            Real-Time Lip Sync
                        </li>
                        <li class="flex items-center gap-2">
                            <i data-lucide="globe" class="w-3 h-3" style="color: #14B8A6;"></i>
                            Multi-Platform Deployment
                        </li>
                    </ul>
                </div>
                
                <!-- Status Section -->
                <div>
                    <h4 class="text-sm font-semibold text-white mb-3">System Status</h4>
                    <div class="space-y-2">
                        <div class="flex items-center justify-between">
                            <span class="text-sm text-gray-400">Avatar System</span>
                            <span class="flex items-center gap-1 text-xs text-green-400">
                                <div class="w-2 h-2 bg-green-400 rounded-full animate-pulse"></div>
                                Active
                            </span>
                        </div>
                        <div class="flex items-center justify-between">
                            <span class="text-sm text-gray-400">Voice Engine</span>
                            <span class="flex items-center gap-1 text-xs text-green-400">
                                <div class="w-2 h-2 bg-green-400 rounded-full animate-pulse"></div>
                                Ready
                            </span>
                        </div>
                        <div class="flex items-center justify-between">
                            <span class="text-sm text-gray-400">API Services</span>
                            <span class="flex items-center gap-1 text-xs text-green-400">
                                <div class="w-2 h-2 bg-green-400 rounded-full animate-pulse"></div>
                                Online
                            </span>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Bottom Bar -->
            <div class="mt-8 pt-6 border-t border-gray-800 flex flex-col md:flex-row justify-between items-center">
                <p class="text-sm text-gray-500">
                    ¬© 2025 SQUAD ONE. Advanced AI Avatar Technology Platform.
                </p>
                <div class="flex items-center gap-4 mt-4 md:mt-0">
                    <span class="text-xs text-gray-600">Powered by</span>
                    <div class="flex items-center gap-3 text-xs text-gray-500">
                        <span>Gemini AI</span>
                        <span>‚Ä¢</span>
                        <span>HuggingFace</span>
                        <span>‚Ä¢</span>
                        <span>LiveKit</span>
                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Initialize Lucide Icons -->
    <script>
        lucide.createIcons();
    </script>
    
    <!-- Initialize THE ISP Controller -->
    <script src="/static/js/the_isp.js"></script>
    <script src="/static/js/advanced_lip_sync.js"></script>
    <script>
        // Initialize THE ISP when page loads
        document.addEventListener('DOMContentLoaded', function() {
            window.ispController = new ISPController();
        });
    </script>
</body>
</html>

=== END FILE ===

=== FILE: static/js/the_isp.js ===
class TheISP {
    constructor() {
        this.livekitRoom = null;
        this.monacoEditor = null;
        this.isConnected = false;
        this.isConversationActive = false;
        this.lipSyncController = null;
        this.advancedLipSync = null;
        this.audioContext = null;
        this.selectedVoiceMode = 'csm'; // Only CSM voice now
        this.faceSwapEnabled = true;
        this.currentPhoneme = 'neutral';
        this.uploadedFiles = {
            instructions: [],
            knowledge: []
        };
        this.customInstructions = [];
        
        this.init();
        this.initializeFileUploads();
        this.initializeInstructionChat();
        this.initializeAdvancedLipSync();
    }
    
    init() {
        this.setupMonacoEditor();
        this.setupEventListeners();
        this.initializeAudioContext();
        this.loadDefaultCode();
    }
    
    setupMonacoEditor() {
        require.config({ paths: { vs: 'https://unpkg.com/monaco-editor@latest/min/vs' } });
        require(['vs/editor/editor.main'], () => {
            this.monacoEditor = monaco.editor.create(document.getElementById('monacoEditor'), {
                value: '// Welcome to THE ISP Programming Playground\n// Start coding your avatar integration here!\n\nconsole.log("Ready to prototype avatar conversations!");',
                language: 'javascript',
                theme: 'vs-dark',
                fontSize: 14,
                minimap: { enabled: false },
                scrollBeyondLastLine: false,
                automaticLayout: true
            });
        });
    }
    
    setupEventListeners() {
        // Tab Navigation
        this.setupTabNavigation();
        
        // Initialize Environment Button
        document.getElementById('initializeBtn').addEventListener('click', () => {
            this.initializeEnvironment();
        });
        
        // Conversation Controls
        document.getElementById('startConversation').addEventListener('click', () => {
            this.startConversation();
        });
        
        document.getElementById('stopConversation').addEventListener('click', () => {
            this.stopConversation();
        });
        
        // Test message buttons with speech - use selected voice mode
        document.querySelectorAll('.test-message').forEach(btn => {
            btn.addEventListener('click', (e) => {
                const message = e.target.getAttribute('data-message');
                this.speakWithSelectedVoice(message);
            });
        });
        
        // Programming Playground Controls
        document.getElementById('runCode').addEventListener('click', () => {
            this.runCode();
        });
        
        document.getElementById('clearCode').addEventListener('click', () => {
            this.clearCode();
        });
        
        // Model Selection
        document.getElementById('playgroundModel').addEventListener('change', (e) => {
            this.changeLanguage(e.target.value);
        });
        
        // Voice controls
        if (document.getElementById('startVoiceTest')) {
            document.getElementById('startVoiceTest').addEventListener('click', () => {
                this.testVoice();
            });
        }
        
        // CSM voice test button (only option now)
        if (document.getElementById('testCSMBtn')) {
            document.getElementById('testCSMBtn').addEventListener('click', () => {
                this.testCSMVoice();
            });
        }
        
        // Audio system test button
        if (document.getElementById('testAudioSystem')) {
            document.getElementById('testAudioSystem').addEventListener('click', () => {
                this.logToConsole('üîä Testing device audio system...');
                this.checkAudioCapabilities();
                this.testAudioPlayback();
                this.speakWithSelectedVoice("Audio test - can you hear this voice clearly through your device speakers? This test verifies your device audio is working properly with face animation.");
            });
        }
        
        // Example Code Buttons
        document.querySelectorAll('.example-code').forEach(btn => {
            btn.addEventListener('click', (e) => {
                const code = e.target.getAttribute('data-code');
                const language = e.target.getAttribute('data-language');
                this.loadExampleCode(code, language);
            });
        });
        
        // Real-time settings updates
        document.getElementById('lipSyncSensitivity').addEventListener('input', (e) => {
            this.updateLipSyncSensitivity(e.target.value);
        });
    }
    
    setupTabNavigation() {
        const tabButtons = document.querySelectorAll('.tab-button');
        const tabContents = document.querySelectorAll('.tab-content');
        
        tabButtons.forEach(button => {
            button.addEventListener('click', () => {
                const tabId = button.id.replace('tab-', 'content-');
                
                // Remove active class from all buttons
                tabButtons.forEach(btn => btn.classList.remove('active'));
                // Add active class to clicked button
                button.classList.add('active');
                
                // Hide all tab contents
                tabContents.forEach(content => content.classList.add('hidden'));
                // Show selected tab content
                const targetContent = document.getElementById(tabId);
                if (targetContent) {
                    targetContent.classList.remove('hidden');
                }
            });
        });
    }
    
    initializeAudioContext() {
        try {
            this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
            
            // Initialize speech synthesis
            this.speechSynth = window.speechSynthesis;
            this.voices = [];
            
            // Load voices when available
            this.loadVoices();
            
            // Reload voices when they change (some browsers load them asynchronously)
            if (speechSynthesis.onvoiceschanged !== undefined) {
                speechSynthesis.onvoiceschanged = () => this.loadVoices();
            }
            
            console.log('Audio context and speech synthesis initialized');
            
            // Initialize voice button states
            setTimeout(() => {
                this.updateVoiceButtonStates();
            }, 500);
        } catch (error) {
            console.error('Failed to initialize audio context:', error);
        }
    }
    
    loadVoices() {
        this.voices = this.speechSynth.getVoices();
        console.log('Voices loaded:', this.voices.length);
        
        // Find different voice types
        this.browserVoice = this.voices.find(voice => 
            voice.name.includes('Google') || 
            voice.name.includes('Microsoft') ||
            voice.name.includes('Alex') ||
            voice.name.includes('Samantha')
        ) || this.voices[0];
        
        // Try to find a more professional/realistic voice for CSM simulation
        this.professionalVoice = this.voices.find(voice => 
            voice.name.includes('Premium') ||
            voice.name.includes('Neural') ||
            voice.name.includes('Enhanced') ||
            voice.name.includes('Natural') ||
            voice.name.includes('Eloquence')
        ) || this.voices.find(voice => voice.name.includes('Microsoft')) || this.browserVoice;
        
        this.selectedVoice = this.browserVoice;
        console.log('Browser voice:', this.browserVoice?.name);
        console.log('Professional voice:', this.professionalVoice?.name);
        
        // Update voice status display
        this.updateVoiceStatusDisplay();
    }
    
    speakText(text, useCSMStyle = false) {
        if (!this.speechSynth) {
            console.error('Speech synthesis not available');
            return;
        }
        
        // Cancel any ongoing speech
        this.speechSynth.cancel();
        
        const utterance = new SpeechSynthesisUtterance(text);
        
        // Choose voice based on mode
        let selectedVoice;
        if (useCSMStyle && this.professionalVoice) {
            selectedVoice = this.professionalVoice;
            utterance.rate = 0.85;  // Slower, more professional
            utterance.pitch = 0.9;  // Slightly lower pitch
            utterance.volume = 0.9; // Higher volume for authority
        } else {
            selectedVoice = this.browserVoice;
            utterance.rate = 0.9;
            utterance.pitch = 1.0;
            utterance.volume = 0.8;
        }
        
        if (selectedVoice) {
            utterance.voice = selectedVoice;
        }
        
        // Add event listeners
        utterance.onstart = () => {
            console.log(`Speech started with ${selectedVoice?.name || 'default'} voice`);
            this.updateStreamStatus('speaking');
            this.startLipSyncAnimation();
        };
        
        utterance.onend = () => {
            console.log('Speech ended');
            this.updateStreamStatus('connected');
            this.stopLipSyncAnimation();
        };
        
        utterance.onerror = (event) => {
            console.error('Speech error:', event.error);
            this.updateStreamStatus('error');
            this.stopLipSyncAnimation();
        };
        
        // Speak the text
        this.speechSynth.speak(utterance);
    }
    
    testVoice() {
        const testMessage = "Hello! I'm your ISP voice agent. Voice synthesis is working perfectly with real-time lip sync animation.";
        this.speakText(testMessage, false);
        this.logToConsole(`üîä Testing Browser Voice: "${testMessage}"`);
    }
    
    async testCSMVoice() {
        const testMessage = "Hello! I'm speaking with the Sesame CSM-1B ultra-realistic voice model. This is 50 years ahead technology with professional quality speech synthesis.";
        await this.speakWithCSM(testMessage);
    }
    
    async speakWithCSM(text) {
        try {
            this.logToConsole(`üé§ Attempting CSM Speech: "${text}"`);
            this.updateStreamStatus('speaking');
            this.startLipSyncAnimation();
            
            const response = await fetch('/api/csm-speech', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    text: text,
                    model: 'sesame/csm-1b'
                })
            });
            
            if (!response.ok) {
                throw new Error(`CSM API error: ${response.status}`);
            }
            
            const audioBlob = await response.blob();
            const audioUrl = URL.createObjectURL(audioBlob);
            const audio = new Audio(audioUrl);
            
            // Setup advanced lip sync with the avatar
            if (this.advancedLipSync) {
                this.advancedLipSync.syncWithCSMAudio(audio);
                this.logToConsole('üé≠ Advanced lip sync activated for avatar');
            }
            
            // Fallback basic lip sync
            this.startLipSyncAnimation();
            
            audio.onended = () => {
                this.updateStreamStatus('connected');
                if (this.advancedLipSync) {
                    this.advancedLipSync.stopLipSync();
                }
                this.stopLipSyncAnimation();
                URL.revokeObjectURL(audioUrl);
            };
            
            audio.onerror = () => {
                this.updateStreamStatus('error');
                if (this.advancedLipSync) {
                    this.advancedLipSync.stopLipSync();
                }
                this.stopLipSyncAnimation();
                this.logToConsole('‚ùå CSM audio playback failed');
            };
            
            await audio.play();
            
        } catch (error) {
            this.logToConsole(`‚ùå CSM-1B API Error: ${error.message}`);
            this.logToConsole(`Please check HuggingFace token and API availability`);
            this.updateStreamStatus('error');
            this.stopLipSyncAnimation();
        }
    }
    
    speakWithSelectedVoice(text) {
        // Use browser voice for immediate testing, CSM when available
        this.speakWithBrowserVoice(text);
    }
    
    speakWithBrowserVoice(text) {
        try {
            this.logToConsole(`üó£Ô∏è Speaking with browser voice: "${text}"`);
            this.updateStreamStatus('speaking');
            
            // Test audio context and user interaction
            this.testAudioPlayback();
            
            // Create speech synthesis
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 0.9;
            utterance.pitch = 1.0;
            utterance.volume = 1.0;
            
            // Get available voices
            const voices = speechSynthesis.getVoices();
            const femaleVoice = voices.find(voice => 
                voice.name.includes('Female') || 
                voice.name.includes('Zira') ||
                voice.name.includes('Google US English') ||
                voice.gender === 'female'
            );
            
            if (femaleVoice) {
                utterance.voice = femaleVoice;
                this.logToConsole(`üé§ Using voice: ${femaleVoice.name}`);
            }
            
            // Start lip sync when speech starts
            utterance.onstart = () => {
                this.logToConsole('üéµ Browser speech started, activating lip sync');
                this.startLipSyncAnimation();
                
                // Start face swap animation for dynamic expressions
                this.startFaceSwapAnimation(text);
                
                // Create fake audio element for advanced lip sync
                const fakeAudio = this.createFakeAudioForLipSync(text.length * 100); // Rough duration estimate
                if (this.advancedLipSync) {
                    this.advancedLipSync.syncWithCSMAudio(fakeAudio);
                }
            };
            
            utterance.onend = () => {
                this.updateStreamStatus('connected');
                this.stopLipSyncAnimation();
                this.stopFaceSwapAnimation();
                if (this.advancedLipSync) {
                    this.advancedLipSync.stopLipSync();
                }
                this.logToConsole('‚úÖ Browser speech completed');
            };
            
            utterance.onerror = (error) => {
                this.updateStreamStatus('error');
                this.stopLipSyncAnimation();
                this.logToConsole(`‚ùå Speech error: ${error.error}`);
            };
            
            // Speak the text
            speechSynthesis.speak(utterance);
            
        } catch (error) {
            this.logToConsole(`‚ùå Browser speech error: ${error.message}`);
            this.updateStreamStatus('error');
        }
    }
    
    createFakeAudioForLipSync(duration) {
        // Create a synthetic audio context for lip sync when using browser speech
        const fakeAudio = {
            addEventListener: (event, callback) => {
                if (event === 'loadstart') setTimeout(callback, 10);
                if (event === 'canplay') setTimeout(callback, 50);
                if (event === 'play') setTimeout(callback, 100);
                if (event === 'ended') setTimeout(callback, duration);
            }
        };
        
        return fakeAudio;
    }
    
    // Face Swap Animation System
    async startFaceSwapAnimation(text) {
        if (!this.faceSwapEnabled) return;
        
        this.logToConsole('üé≠ Starting dynamic face animation');
        this.isAnimatingFace = true;
        
        // Create phoneme sequence from text
        const phonemes = this.textToPhonemes(text);
        
        // Animate through phonemes
        let i = 0;
        const animatePhoneme = async () => {
            if (!this.isAnimatingFace) return;
            
            const phoneme = phonemes[i % phonemes.length];
            const intensity = 0.3 + Math.random() * 0.4;
            
            await this.updateAvatarFace(phoneme, intensity);
            
            i++;
            setTimeout(animatePhoneme, 150 + Math.random() * 100); // Vary timing naturally
        };
        
        animatePhoneme();
    }
    
    stopFaceSwapAnimation() {
        this.isAnimatingFace = false;
        this.logToConsole('üé≠ Face animation stopped');
        
        // Return to neutral expression
        setTimeout(() => {
            this.updateAvatarFace('neutral', 0.2);
        }, 500);
    }
    
    textToPhonemes(text) {
        // Convert text to phoneme sequence for realistic mouth movements
        const words = text.toLowerCase().split(/\s+/);
        const phonemes = [];
        
        for (const word of words) {
            for (const char of word) {
                if ('aeiou'.includes(char)) {
                    phonemes.push(char);
                } else if ('mbp'.includes(char)) {
                    phonemes.push('m');
                } else if ('fv'.includes(char)) {
                    phonemes.push('f');
                } else if ('sz'.includes(char)) {
                    phonemes.push('s');
                } else {
                    phonemes.push('a'); // Default open mouth
                }
            }
            phonemes.push('neutral'); // Pause between words
        }
        
        return phonemes;
    }
    
    async updateAvatarFace(phoneme, intensity) {
        try {
            const response = await fetch('/api/face-swap', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    phoneme: phoneme,
                    emotion: 'friendly',
                    intensity: intensity
                })
            });
            
            if (response.ok) {
                const data = await response.json();
                if (data.success && data.image) {
                    // Update avatar image with new dynamic face
                    const avatarImg = document.getElementById('avatar-image');
                    if (avatarImg) {
                        avatarImg.src = `data:image/jpeg;base64,${data.image}`;
                        this.currentPhoneme = phoneme;
                    }
                } else {
                    // Fallback to overlay animation
                    this.updateMouthOverlay(phoneme, intensity);
                }
            } else {
                // Fallback to overlay animation
                this.updateMouthOverlay(phoneme, intensity);
            }
        } catch (error) {
            // Silent fallback to overlay animation
            this.updateMouthOverlay(phoneme, intensity);
        }
    }
    
    updateMouthOverlay(phoneme, intensity) {
        // Fallback mouth overlay animation when face swap isn't available
        const avatarContainer = document.querySelector('.avatar-display');
        if (!avatarContainer) return;
        
        let mouthOverlay = avatarContainer.querySelector('.mouth-overlay');
        if (!mouthOverlay) {
            mouthOverlay = document.createElement('div');
            mouthOverlay.className = 'mouth-overlay';
            mouthOverlay.style.cssText = `
                position: absolute;
                bottom: 30%;
                left: 50%;
                background: rgba(20, 20, 20, 0.8);
                border-radius: 50%;
                transform: translate(-50%, -50%);
                transition: all 0.1s ease;
                z-index: 10;
            `;
            avatarContainer.appendChild(mouthOverlay);
        }
        
        // Update mouth shape based on phoneme
        const shapes = {
            'a': { width: '12px', height: '8px' },
            'e': { width: '10px', height: '4px' },
            'i': { width: '6px', height: '3px' },
            'o': { width: '8px', height: '8px' },
            'u': { width: '4px', height: '6px' },
            'm': { width: '2px', height: '1px' },
            'f': { width: '8px', height: '2px' },
            's': { width: '6px', height: '2px' },
            'neutral': { width: '1px', height: '1px' }
        };
        
        const shape = shapes[phoneme] || shapes['a'];
        const scale = 0.8 + (intensity * 0.4);
        
        mouthOverlay.style.width = `${parseInt(shape.width) * scale}px`;
        mouthOverlay.style.height = `${parseInt(shape.height) * scale}px`;
    }
    
    // Audio Testing System
    testAudioPlayback() {
        try {
            // Test if audio context is active
            if (this.audioContext && this.audioContext.state === 'suspended') {
                this.audioContext.resume().then(() => {
                    this.logToConsole('üîä Audio context resumed');
                });
            }
            
            // Test with a brief tone to verify device audio
            this.playTestTone();
            
            // Verify speech synthesis is available
            if (!window.speechSynthesis) {
                throw new Error('Speech synthesis not supported');
            }
            
            this.logToConsole('‚úÖ Audio system check passed');
            
        } catch (error) {
            this.logToConsole(`‚ùå Audio test failed: ${error.message}`);
        }
    }
    
    playTestTone() {
        try {
            // Create audio context if not exists
            if (!this.audioContext) {
                this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
            
            // Create a brief 440Hz tone (A note) to test audio output
            const oscillator = this.audioContext.createOscillator();
            const gainNode = this.audioContext.createGain();
            
            oscillator.connect(gainNode);
            gainNode.connect(this.audioContext.destination);
            
            oscillator.frequency.setValueAtTime(440, this.audioContext.currentTime);
            gainNode.gain.setValueAtTime(0.1, this.audioContext.currentTime); // Low volume
            gainNode.gain.exponentialRampToValueAtTime(0.01, this.audioContext.currentTime + 0.1);
            
            oscillator.start(this.audioContext.currentTime);
            oscillator.stop(this.audioContext.currentTime + 0.1);
            
            this.logToConsole('üéµ Audio test tone played');
            
        } catch (error) {
            this.logToConsole(`‚ö†Ô∏è Test tone failed: ${error.message}`);
        }
    }
    
    // Enhanced audio diagnostics
    checkAudioCapabilities() {
        const capabilities = {
            speechSynthesis: !!window.speechSynthesis,
            audioContext: !!(window.AudioContext || window.webkitAudioContext),
            webAudio: typeof Audio !== 'undefined',
            voices: window.speechSynthesis ? speechSynthesis.getVoices().length : 0,
            audioContextState: this.audioContext ? this.audioContext.state : 'none'
        };
        
        this.logToConsole('üîç Audio Capabilities:');
        Object.entries(capabilities).forEach(([key, value]) => {
            this.logToConsole(`  ${key}: ${value}`);
        });
        
        return capabilities;
    }
    
    updateVoiceButtonStates() {
        const browserBtn = document.getElementById('testVoiceBtn');
        const csmBtn = document.getElementById('testCSMBtn');
        
        if (browserBtn && csmBtn) {
            if (this.selectedVoiceMode === 'browser') {
                browserBtn.style.backgroundColor = '#DAA520';
                browserBtn.style.color = '#000000';
                browserBtn.style.fontWeight = 'bold';
                browserBtn.textContent = 'üîä Browser Voice (Active)';
                
                csmBtn.style.backgroundColor = '#8B4513';
                csmBtn.style.color = '#FFFFFF';
                csmBtn.style.fontWeight = 'normal';
                csmBtn.textContent = 'üé§ Test CSM-1B Voice';
            } else {
                browserBtn.style.backgroundColor = '#DAA520';
                browserBtn.style.color = '#000000';
                browserBtn.style.fontWeight = 'normal';
                browserBtn.textContent = 'üîä Test Browser Voice';
                
                csmBtn.style.backgroundColor = '#8B4513';
                csmBtn.style.color = '#FFFFFF';
                csmBtn.style.fontWeight = 'bold';
                csmBtn.textContent = 'üé§ CSM-1B Voice (Active)';
            }
        }
    }
    
    updateVoiceStatusDisplay() {
        const voiceInfo = document.getElementById('voiceInfo');
        if (voiceInfo && this.voices.length > 0) {
            const browserVoiceName = this.browserVoice?.name || 'Default';
            const professionalVoiceName = this.professionalVoice?.name || 'Default';
            
            voiceInfo.innerHTML = `
                <div class="mb-1">üîä Browser: ${browserVoiceName}</div>
                <div class="mb-1">üé§ Professional: ${professionalVoiceName}</div>
                <div class="text-xs text-gray-500">Total voices available: ${this.voices.length}</div>
            `;
        }
    }
    
    startLipSyncAnimation() {
        const mouthAnimation = document.getElementById('mouthAnimation');
        const lipSyncBar = document.getElementById('lipSyncBar');
        
        if (mouthAnimation) {
            mouthAnimation.style.opacity = '1';
            
            // Animate mouth movement
            this.lipSyncInterval = setInterval(() => {
                const intensity = Math.random() * 100;
                mouthAnimation.style.transform = `translateX(-50%) scaleY(${0.5 + intensity/200})`;
                
                if (lipSyncBar) {
                    lipSyncBar.style.width = `${intensity}%`;
                }
            }, 100);
        }
    }
    
    stopLipSyncAnimation() {
        const mouthAnimation = document.getElementById('mouthAnimation');
        const lipSyncBar = document.getElementById('lipSyncBar');
        
        if (this.lipSyncInterval) {
            clearInterval(this.lipSyncInterval);
            this.lipSyncInterval = null;
        }
        
        if (mouthAnimation) {
            mouthAnimation.style.opacity = '0';
            mouthAnimation.style.transform = 'translateX(-50%) scaleY(1)';
        }
        
        if (lipSyncBar) {
            lipSyncBar.style.width = '0%';
        }
    }
    
    updateStreamStatus(status) {
        const indicator = document.getElementById('streamStatus');
        if (indicator) {
            indicator.className = `status-indicator status-${status}`;
        }
    }
    
    async initializeEnvironment() {
        this.updateStatus('Initializing environment...', 'streaming');
        
        try {
            // Validate required credentials
            const livekitUrl = document.getElementById('livekitUrl').value;
            const livekitApiKey = document.getElementById('livekitApiKey').value;
            const livekitApiSecret = document.getElementById('livekitApiSecret').value;
            const hfToken = document.getElementById('hfToken').value;
            
            if (!livekitUrl || !livekitApiKey || !livekitApiSecret || !hfToken) {
                throw new Error('Please fill in all required credentials');
            }
            
            // Initialize LiveKit connection
            await this.initializeLiveKit(livekitUrl, livekitApiKey, livekitApiSecret);
            
            // Initialize HuggingFace connection
            await this.initializeHuggingFace(hfToken);
            
            this.updateStatus('Environment ready', 'connected');
            this.isConnected = true;
            
            // Enable conversation controls
            document.getElementById('startConversation').disabled = false;
            
            this.logToConsole('‚úÖ Environment initialized successfully!');
            this.logToConsole('üé• LiveKit connected');
            this.logToConsole('ü§ñ HuggingFace streaming ready');
            this.logToConsole('üé§ Audio systems active');
            
        } catch (error) {
            this.updateStatus('Initialization failed', 'disconnected');
            this.logToConsole(`‚ùå Error: ${error.message}`);
            console.error('Initialization error:', error);
        }
    }
    
    async initializeLiveKit(url, apiKey, apiSecret) {
        try {
            // Note: In a real implementation, you would generate the token server-side
            // This is a simplified example for prototype purposes
            this.logToConsole('üîó Connecting to LiveKit...');
            
            // Simulated LiveKit initialization for prototype
            // In production, replace with actual LiveKit SDK calls
            setTimeout(() => {
                document.getElementById('streamStatus').className = 'status-indicator status-connected';
                this.logToConsole('üì° LiveKit room connected');
            }, 1000);
            
        } catch (error) {
            throw new Error(`LiveKit initialization failed: ${error.message}`);
        }
    }
    
    async initializeHuggingFace(token) {
        try {
            this.logToConsole('ü§ñ Initializing HuggingFace streaming...');
            
            // Test HuggingFace API connection
            const model = document.getElementById('hfModel').value;
            const response = await fetch(`https://api-inference.huggingface.co/models/${model}`, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${token}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    inputs: "Hello, this is a connection test.",
                    parameters: { max_length: 50 }
                })
            });
            
            if (!response.ok) {
                throw new Error(`HuggingFace API error: ${response.status}`);
            }
            
            this.logToConsole('‚úÖ HuggingFace API connected');
            
        } catch (error) {
            throw new Error(`HuggingFace initialization failed: ${error.message}`);
        }
    }
    
    startConversation() {
        this.isConversationActive = true;
        document.getElementById('startConversation').disabled = true;
        document.getElementById('stopConversation').disabled = false;
        
        this.updateStreamStatus('connected');
        this.logToConsole('üéôÔ∏è Conversation started - Voice system ready');
        this.logToConsole('üí° Click any test message to hear the avatar speak');
        this.logToConsole('üîä Voice synthesis active with lip sync animation');
        
        // Start lip sync visualization
        this.startLipSyncVisualization();
    }
    
    stopConversation() {
        this.isConversationActive = false;
        document.getElementById('startConversation').disabled = false;
        document.getElementById('stopConversation').disabled = true;
        
        this.updateStreamStatus('disconnected');
        this.logToConsole('‚èπÔ∏è Conversation stopped');
        
        // Stop any ongoing speech and lip sync
        if (this.speechSynth) {
            this.speechSynth.cancel();
        }
        this.stopLipSyncAnimation();
        this.stopLipSyncVisualization();
    }
    
    async startAudioCapture() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            this.lipSyncController.startLipSync(stream);
            this.logToConsole('üé§ Audio capture started');
        } catch (error) {
            this.logToConsole(`‚ùå Audio capture failed: ${error.message}`);
        }
    }
    
    stopAudioCapture() {
        if (this.lipSyncController) {
            this.lipSyncController.stopLipSync();
        }
        this.logToConsole('üîá Audio capture stopped');
    }
    
    startLipSyncVisualization() {
        const mouthAnimation = document.getElementById('mouthAnimation');
        const avatarImage = document.getElementById('avatarImage');
        
        // Enhanced lip sync with avatar facial animation
        this.lipSyncInterval = setInterval(() => {
            if (this.isConversationActive) {
                const activity = Math.random() * 100;
                const lipSyncBar = document.getElementById('lipSyncBar');
                lipSyncBar.style.width = activity + '%';
                
                // Animate mouth overlay and facial features
                const intensity = activity / 100;
                mouthAnimation.style.opacity = intensity * 0.6;
                mouthAnimation.style.transform = `translateX(-50%) scale(${1 + intensity * 0.3})`;
                
                // Subtle facial brightness changes
                if (avatarImage) {
                    avatarImage.style.filter = `brightness(${1 + intensity * 0.08}) saturate(${1 + intensity * 0.1})`;
                }
            }
        }, 100);
    }
    
    stopLipSyncVisualization() {
        if (this.lipSyncInterval) {
            clearInterval(this.lipSyncInterval);
        }
        
        // Reset all animations
        document.getElementById('lipSyncBar').style.width = '0%';
        
        const mouthAnimation = document.getElementById('mouthAnimation');
        const avatarImage = document.getElementById('avatarImage');
        
        if (mouthAnimation) {
            mouthAnimation.style.opacity = '0';
            mouthAnimation.style.transform = 'translateX(-50%) scale(1)';
        }
        
        if (avatarImage) {
            avatarImage.style.filter = 'brightness(1) saturate(1) contrast(1)';
        }
    }
    
    async sendTestMessage(message) {
        if (!this.isConversationActive) {
            this.logToConsole('‚ùå Start conversation first');
            return;
        }
        
        this.logToConsole(`üì§ Sending: "${message}"`);
        
        try {
            const hfToken = document.getElementById('hfToken').value;
            const model = document.getElementById('hfModel').value;
            
            const response = await fetch(`https://api-inference.huggingface.co/models/${model}`, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${hfToken}`,
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    inputs: message,
                    parameters: {
                        max_length: 100,
                        temperature: 0.7,
                        do_sample: true
                    }
                })
            });
            
            const data = await response.json();
            
            if (data.error) {
                throw new Error(data.error);
            }
            
            const aiResponse = data[0]?.generated_text || 'No response generated';
            this.logToConsole(`üì• AI Response: "${aiResponse}"`);
            
            // Simulate lip sync during response
            this.simulateResponseLipSync();
            
        } catch (error) {
            this.logToConsole(`‚ùå Message failed: ${error.message}`);
        }
    }
    
    simulateResponseLipSync() {
        let duration = 0;
        const maxDuration = 3000; // 3 seconds
        const mouthAnimation = document.getElementById('mouthAnimation');
        
        const animate = () => {
            if (duration < maxDuration && this.isConversationActive) {
                // Update progress bar
                const activity = 30 + Math.sin(duration / 100) * 40 + Math.random() * 30;
                document.getElementById('lipSyncBar').style.width = activity + '%';
                
                // Animate mouth overlay on avatar image
                const mouthIntensity = activity / 100;
                mouthAnimation.style.opacity = mouthIntensity * 0.8;
                mouthAnimation.style.transform = `translateX(-50%) scale(${1 + mouthIntensity * 0.5})`;
                
                // Add slight facial animation
                const avatarImage = document.getElementById('avatarImage');
                if (avatarImage) {
                    avatarImage.style.filter = `brightness(${1 + mouthIntensity * 0.1}) contrast(${1 + mouthIntensity * 0.05})`;
                }
                
                duration += 50;
                setTimeout(animate, 50);
            } else {
                // Reset animations
                document.getElementById('lipSyncBar').style.width = '0%';
                mouthAnimation.style.opacity = '0';
                mouthAnimation.style.transform = 'translateX(-50%) scale(1)';
                
                const avatarImage = document.getElementById('avatarImage');
                if (avatarImage) {
                    avatarImage.style.filter = 'brightness(1) contrast(1)';
                }
            }
        };
        
        animate();
    }
    
    runCode() {
        if (!this.monacoEditor) {
            this.logToConsole('‚ùå Editor not ready');
            return;
        }
        
        const code = this.monacoEditor.getValue();
        const language = document.getElementById('playgroundModel').value;
        
        this.logToConsole(`üöÄ Running ${language} code...`);
        this.logToConsole('‚îÄ'.repeat(40));
        
        try {
            if (language === 'javascript') {
                // Create a safe execution context
                const consoleOutput = document.getElementById('consoleOutput');
                const originalLog = console.log;
                
                // Capture console.log output
                console.log = (...args) => {
                    this.logToConsole(args.join(' '));
                    originalLog.apply(console, args);
                };
                
                // Execute the code
                eval(code);
                
                // Restore original console.log
                console.log = originalLog;
                
            } else {
                this.logToConsole(`‚ö†Ô∏è ${language} execution not implemented in browser`);
                this.logToConsole('üí° Use JavaScript for interactive execution');
            }
            
        } catch (error) {
            this.logToConsole(`‚ùå Runtime Error: ${error.message}`);
        }
        
        this.logToConsole('‚îÄ'.repeat(40));
    }
    
    clearCode() {
        if (this.monacoEditor) {
            this.monacoEditor.setValue('');
        }
    }
    
    loadDefaultCode() {
        const defaultCode = {
            javascript: `// THE ISP - Avatar Conversation Prototype
// MCP (Model Context Protocol) + LiveKit for real-time avatars

// Fast MCP Streaming for Avatar Conversations
class MCPAvatarConversation {
    constructor() {
        this.isActive = false;
        this.mcpUrl = 'http://localhost:9000/mcp';
        this.lipSyncEnabled = true;
    }
    
    async startMCPServer() {
        console.log("üé¨ Starting MCP avatar conversation...");
        console.log("üí° Start MCP server: ollama-mcp serve --model 'llama3:8b-instruct' --port 9000");
        this.isActive = true;
    }
    
    // One JSON-RPC call for 50-100ms GPU latency
    async mcpStreamResponse(message) {
        const response = await fetch(this.mcpUrl, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                jsonrpc: '2.0',
                id: 1,
                method: 'generate',
                params: {
                    prompt: message,
                    max_tokens: 120,
                    temperature: 0.7,
                    stream: true  // Streaming for real-time UI
                }
            })
        });
        
        const data = await response.json();
        console.log(\`üöÄ MCP Response (50ms): \${data.result?.text || 'Connected!'}\`);
        return data;
    }
    
    // Alternative: HuggingFace with latest models
    async hfChatTemplate(message) {
        const messages = [
            {"role": "system", "content": "You are a helpful avatar assistant."},
            {"role": "user", "content": message}
        ];
        
        console.log(\`üì§ HF Chat Template: \${message}\`);
        
        // Simulate response - replace with actual HF API
        const responses = [
            "I understand. Let me help you with that.",
            "That's a great question! Here's my perspective...",
            "I can definitely assist you with this.",
            "Interesting point! Let me think about that..."
        ];
        
        const response = responses[Math.floor(Math.random() * responses.length)];
        console.log(\`ü§ñ Avatar Response: \${response}\`);
        return response;
    }
    
    // vLLM for fastest inference (3-5x faster than transformers)
    async vllmGenerate(prompt) {
        console.log("‚ö° vLLM fastest inference mode");
        console.log("pip install vllm");
        console.log("from vllm import LLM, SamplingParams");
        console.log(\`Processing: "\${prompt}"\`);
        
        // Simulate ultra-fast response
        setTimeout(() => {
            console.log("‚úÖ vLLM response in ~20ms with continuous batching");
        }, 20);
    }
}

// Initialize and test all approaches
const mcpAvatar = new MCPAvatarConversation();
mcpAvatar.startMCPServer();
mcpAvatar.mcpStreamResponse("Explain SaaS churn prediction in 100 words.");
mcpAvatar.hfChatTemplate("How do I build SaaS fast?");
mcpAvatar.vllmGenerate("Explain quantum field theory in one line");

console.log("üî• All avatar conversation methods ready!");`,
            
            python: `# THE ISP - Avatar Conversation Prototype
# Python backend for avatar processing

import asyncio
import json

class AvatarProcessor:
    def __init__(self):
        self.is_active = False
        self.lip_sync_enabled = True
    
    async def start_session(self):
        print("üé¨ Starting avatar session...")
        self.is_active = True
        print("‚úÖ Avatar processor ready!")
    
    async def process_message(self, user_input):
        if not self.is_active:
            return
        
        print(f"üì§ Processing: {user_input}")
        
        # Simulate AI processing
        responses = [
            "That's fascinating!",
            "I see what you mean.",
            "Let me consider that.",
            "Excellent question!"
        ]
        
        import random
        response = random.choice(responses)
        print(f"ü§ñ Response: {response}")
        
        return response

# Example usage
avatar = AvatarProcessor()
print("Avatar processor initialized!")`,
            
            html: `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Avatar Display</title>
    <style>
        body {
            background: #000;
            color: #fff;
            font-family: 'Inter', sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
        }
        
        .avatar-container {
            background: linear-gradient(145deg, #1a1a1a, #2d2d2d);
            border-radius: 20px;
            padding: 30px;
            text-align: center;
            border: 1px solid #333;
        }
        
        .avatar-placeholder {
            width: 200px;
            height: 200px;
            background: linear-gradient(45deg, #14B8A6, #0f9486);
            border-radius: 50%;
            margin: 0 auto 20px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 48px;
        }
        
        .lip-sync-indicator {
            width: 100%;
            height: 20px;
            background: #333;
            border-radius: 10px;
            overflow: hidden;
            margin-top: 20px;
        }
        
        .lip-sync-bar {
            height: 100%;
            background: linear-gradient(90deg, #14B8A6, #DAA520);
            width: 0%;
            transition: width 0.1s ease;
            border-radius: 10px;
        }
    </style>
</head>
<body>
    <div class="avatar-container">
        <h2 style="color: #DAA520;">Avatar Display</h2>
        <div class="avatar-placeholder">ü§ñ</div>
        <p>Interactive Avatar Interface</p>
        <div class="lip-sync-indicator">
            <div class="lip-sync-bar" id="lipSyncBar"></div>
        </div>
    </div>
    
    <script>
        // Simulate lip sync animation
        setInterval(() => {
            const bar = document.getElementById('lipSyncBar');
            const width = Math.random() * 80;
            bar.style.width = width + '%';
        }, 200);
    </script>
</body>
</html>`
        };
        
        setTimeout(() => {
            if (this.monacoEditor) {
                this.monacoEditor.setValue(defaultCode.javascript);
            }
        }, 1000);
    }
    
    changeLanguage(language) {
        if (!this.monacoEditor) return;
        
        const languageMap = {
            javascript: 'javascript',
            python: 'python',
            typescript: 'typescript',
            html: 'html'
        };
        
        monaco.editor.setModelLanguage(this.monacoEditor.getModel(), languageMap[language] || 'javascript');
        this.logToConsole(`üîÑ Switched to ${language} mode`);
    }
    
    loadExampleCode(code, language) {
        if (this.monacoEditor) {
            this.monacoEditor.setValue(code);
            this.changeLanguage(language);
            this.logToConsole(`üìã Loaded ${language} example`);
        }
    }
    
    updateLipSyncSensitivity(value) {
        this.logToConsole(`üéöÔ∏è Lip sync sensitivity: ${value}%`);
    }
    
    updateStatus(message, type) {
        const statusElement = document.getElementById('statusText');
        const indicatorElement = document.getElementById('connectionStatus');
        
        statusElement.textContent = message;
        indicatorElement.className = `status-indicator status-${type}`;
    }
    
    logToConsole(message) {
        const console = document.getElementById('consoleOutput');
        const timestamp = new Date().toLocaleTimeString();
        
        const logEntry = document.createElement('div');
        logEntry.innerHTML = `<span class="text-muted">[${timestamp}]</span> ${message}`;
        
        console.appendChild(logEntry);
        console.scrollTop = console.scrollHeight;
    }
    
    // File Upload System Methods
    initializeFileUploads() {
        // Instructions file upload
        const instructionsDropZone = document.getElementById('instructionsDropZone');
        const instructionsFileInput = document.getElementById('instructionsFileInput');
        
        if (instructionsDropZone && instructionsFileInput) {
            instructionsDropZone.addEventListener('click', () => instructionsFileInput.click());
            instructionsFileInput.addEventListener('change', (e) => this.handleFileUpload(e.target.files, 'instructions'));
            
            instructionsDropZone.addEventListener('dragover', (e) => {
                e.preventDefault();
                instructionsDropZone.style.borderColor = '#14B8A6';
            });
            instructionsDropZone.addEventListener('dragleave', (e) => {
                e.preventDefault();
                instructionsDropZone.style.borderColor = '#6B7280';
            });
            instructionsDropZone.addEventListener('drop', (e) => {
                e.preventDefault();
                instructionsDropZone.style.borderColor = '#6B7280';
                this.handleFileUpload(e.dataTransfer.files, 'instructions');
            });
        }
        
        // Knowledge base file upload
        const knowledgeDropZone = document.getElementById('knowledgeDropZone');
        const knowledgeFileInput = document.getElementById('knowledgeFileInput');
        
        if (knowledgeDropZone && knowledgeFileInput) {
            knowledgeDropZone.addEventListener('click', () => knowledgeFileInput.click());
            knowledgeFileInput.addEventListener('change', (e) => this.handleFileUpload(e.target.files, 'knowledge'));
            
            knowledgeDropZone.addEventListener('dragover', (e) => {
                e.preventDefault();
                knowledgeDropZone.style.borderColor = '#14B8A6';
            });
            knowledgeDropZone.addEventListener('dragleave', (e) => {
                e.preventDefault();
                knowledgeDropZone.style.borderColor = '#6B7280';
            });
            knowledgeDropZone.addEventListener('drop', (e) => {
                e.preventDefault();
                knowledgeDropZone.style.borderColor = '#6B7280';
                this.handleFileUpload(e.dataTransfer.files, 'knowledge');
            });
        }
        
        // Process and clear buttons
        if (document.getElementById('processFiles')) {
            document.getElementById('processFiles').addEventListener('click', () => this.processUploadedFiles());
        }
        if (document.getElementById('clearFiles')) {
            document.getElementById('clearFiles').addEventListener('click', () => this.clearUploadedFiles());
        }
    }
    
    handleFileUpload(files, type) {
        Array.from(files).forEach(file => {
            if (this.validateFile(file, type)) {
                this.uploadedFiles[type].push(file);
                this.displayUploadedFile(file, type);
                this.logToConsole(`üìÅ Uploaded ${type} file: ${file.name}`);
            }
        });
        this.updateProcessingStatus();
    }
    
    validateFile(file, type) {
        const maxSize = 10 * 1024 * 1024; // 10MB
        const instructionTypes = ['.txt', '.md', '.json', '.yaml', '.yml'];
        const knowledgeTypes = ['.pdf', '.docx', '.txt', '.csv', '.json'];
        
        const allowedTypes = type === 'instructions' ? instructionTypes : knowledgeTypes;
        const extension = file.name.toLowerCase().substring(file.name.lastIndexOf('.'));
        
        if (!allowedTypes.includes(extension)) {
            this.logToConsole(`‚ùå Invalid file type: ${file.name}`);
            return false;
        }
        
        if (file.size > maxSize) {
            this.logToConsole(`‚ùå File too large: ${file.name} (max 10MB)`);
            return false;
        }
        
        return true;
    }
    
    displayUploadedFile(file, type) {
        const listElement = document.getElementById(`${type}FileList`);
        if (!listElement) return;
        
        const fileItem = document.createElement('div');
        fileItem.className = 'flex items-center justify-between bg-gray-700 px-3 py-2 rounded text-xs';
        fileItem.innerHTML = `
            <div class="flex items-center gap-2">
                <i data-lucide="file" class="w-3 h-3 text-gray-400"></i>
                <span class="text-gray-300">${file.name}</span>
                <span class="text-gray-500">(${(file.size / 1024).toFixed(1)}KB)</span>
            </div>
            <button class="text-red-400 hover:text-red-300" onclick="window.theISP.removeFile('${file.name}', '${type}')">
                <i data-lucide="x" class="w-3 h-3"></i>
            </button>
        `;
        
        listElement.appendChild(fileItem);
        if (typeof lucide !== 'undefined') lucide.createIcons();
    }
    
    removeFile(fileName, type) {
        this.uploadedFiles[type] = this.uploadedFiles[type].filter(file => file.name !== fileName);
        const listElement = document.getElementById(`${type}FileList`);
        if (listElement) {
            listElement.innerHTML = '';
            this.uploadedFiles[type].forEach(file => this.displayUploadedFile(file, type));
        }
        this.updateProcessingStatus();
        this.logToConsole(`üóëÔ∏è Removed ${type} file: ${fileName}`);
    }
    
    async processUploadedFiles() {
        const totalFiles = this.uploadedFiles.instructions.length + this.uploadedFiles.knowledge.length;
        if (totalFiles === 0) {
            this.logToConsole(`‚ö†Ô∏è No files to process`);
            return;
        }
        
        this.updateProcessingStatus('Processing files...');
        this.logToConsole(`üß† Processing ${totalFiles} files...`);
        
        try {
            for (const file of this.uploadedFiles.instructions) {
                const content = await this.readFileContent(file);
                this.logToConsole(`üìã Processed instruction: ${file.name} (${content.length} chars)`);
            }
            
            for (const file of this.uploadedFiles.knowledge) {
                const content = await this.readFileContent(file);
                this.logToConsole(`üìö Processed knowledge: ${file.name} (${content.length} chars)`);
            }
            
            this.updateProcessingStatus('‚úÖ All files processed successfully');
            this.logToConsole(`‚úÖ Processed ${totalFiles} files successfully`);
        } catch (error) {
            this.updateProcessingStatus(`‚ùå Processing failed: ${error.message}`);
            this.logToConsole(`‚ùå Processing error: ${error.message}`);
        }
    }
    
    readFileContent(file) {
        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onload = (e) => resolve(e.target.result);
            reader.onerror = (e) => reject(new Error('Failed to read file'));
            reader.readAsText(file);
        });
    }
    
    clearUploadedFiles() {
        this.uploadedFiles.instructions = [];
        this.uploadedFiles.knowledge = [];
        
        ['instructions', 'knowledge'].forEach(type => {
            const listElement = document.getElementById(`${type}FileList`);
            if (listElement) listElement.innerHTML = '';
        });
        
        this.updateProcessingStatus('Ready to process uploaded files...');
        this.logToConsole(`üóëÔ∏è Cleared all uploaded files`);
    }
    
    updateProcessingStatus(message = null) {
        const statusElement = document.getElementById('processingStatus');
        if (!statusElement) return;
        
        if (message) {
            statusElement.textContent = message;
        } else {
            const totalFiles = this.uploadedFiles.instructions.length + this.uploadedFiles.knowledge.length;
            if (totalFiles === 0) {
                statusElement.textContent = 'Ready to process uploaded files...';
            } else {
                statusElement.textContent = `${totalFiles} files ready (${this.uploadedFiles.instructions.length} instructions, ${this.uploadedFiles.knowledge.length} knowledge)`;
            }
        }
    }
    
    // Initialize Advanced Lip Sync System
    initializeAdvancedLipSync() {
        if (typeof AdvancedLipSync !== 'undefined') {
            this.advancedLipSync = new AdvancedLipSync();
            this.logToConsole('üé≠ Advanced lip sync system initialized');
        }
    }
    
    // Initialize Instruction Chat System
    initializeInstructionChat() {
        const chatInput = document.getElementById('instructionChatInput');
        const addBtn = document.getElementById('addInstructionBtn');
        
        if (chatInput && addBtn) {
            // Add instruction on button click
            addBtn.addEventListener('click', () => {
                this.addCustomInstruction();
            });
            
            // Add instruction on Enter key
            chatInput.addEventListener('keypress', (e) => {
                if (e.key === 'Enter') {
                    this.addCustomInstruction();
                }
            });
            
            // Auto-resize and character counter
            chatInput.addEventListener('input', (e) => {
                const remaining = 200 - e.target.value.length;
                if (remaining < 20) {
                    chatInput.style.borderColor = remaining < 0 ? '#ef4444' : '#f59e0b';
                } else {
                    chatInput.style.borderColor = '#4b5563';
                }
            });
        }
    }
    
    addCustomInstruction() {
        const chatInput = document.getElementById('instructionChatInput');
        const messagesContainer = document.getElementById('instructionChatMessages');
        
        if (!chatInput || !messagesContainer) return;
        
        const instruction = chatInput.value.trim();
        if (!instruction || instruction.length > 200) return;
        
        // Add to custom instructions array
        this.customInstructions.push({
            text: instruction,
            timestamp: new Date().toLocaleTimeString(),
            id: Date.now()
        });
        
        // Clear input
        chatInput.value = '';
        chatInput.style.borderColor = '#4b5563';
        
        // Update chat display
        this.updateInstructionChat();
        
        // Log the addition
        this.logToConsole(`üìù Added instruction: "${instruction}"`);
    }
    
    updateInstructionChat() {
        const messagesContainer = document.getElementById('instructionChatMessages');
        if (!messagesContainer) return;
        
        if (this.customInstructions.length === 0) {
            messagesContainer.innerHTML = `
                <div class="text-gray-500 text-xs text-center">
                    Describe how you want the avatar to behave, speak, and respond...
                </div>
            `;
            return;
        }
        
        const messagesHtml = this.customInstructions.map(instruction => `
            <div class="bg-gray-700 rounded-lg p-2 relative group">
                <div class="text-white text-xs mb-1">${instruction.text}</div>
                <div class="flex justify-between items-center">
                    <span class="text-gray-400 text-xs">${instruction.timestamp}</span>
                    <button 
                        onclick="window.theISP.removeCustomInstruction(${instruction.id})"
                        class="opacity-0 group-hover:opacity-100 text-red-400 hover:text-red-300 text-xs transition-opacity"
                    >
                        <i data-lucide="x" class="w-3 h-3"></i>
                    </button>
                </div>
            </div>
        `).join('');
        
        messagesContainer.innerHTML = messagesHtml;
        
        // Re-initialize Lucide icons
        if (typeof lucide !== 'undefined') {
            lucide.createIcons();
        }
        
        // Scroll to bottom
        messagesContainer.scrollTop = messagesContainer.scrollHeight;
    }
    
    removeCustomInstruction(id) {
        this.customInstructions = this.customInstructions.filter(instruction => instruction.id !== id);
        this.updateInstructionChat();
        this.logToConsole(`üóëÔ∏è Removed custom instruction`);
    }
    
    getCustomInstructionsText() {
        if (this.customInstructions.length === 0) return '';
        
        const instructionsText = this.customInstructions
            .map(instruction => instruction.text)
            .join(' | ');
        
        return `Custom Personality: ${instructionsText}`;
    }
}

// Lip Sync Controller Class
class LipSyncController {
    constructor(audioContext) {
        this.audioContext = audioContext;
        this.analyser = null;
        this.isAnimating = false;
        this.animationFrame = null;
    }
    
    startLipSync(audioStream) {
        if (!this.audioContext || !audioStream) return;
        
        try {
            const source = this.audioContext.createMediaStreamSource(audioStream);
            this.analyser = this.audioContext.createAnalyser();
            source.connect(this.analyser);
            
            this.analyser.fftSize = 256;
            const bufferLength = this.analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            
            const mouthAnimation = document.getElementById('mouthAnimation');
            const avatarImage = document.getElementById('avatarImage');
            
            const animate = () => {
                if (!this.isAnimating) return;
                
                this.analyser.getByteFrequencyData(dataArray);
                const average = dataArray.reduce((a, b) => a + b) / bufferLength;
                
                // Update lip sync visualization bar
                const lipSyncBar = document.getElementById('lipSyncBar');
                const normalizedValue = (average / 255) * 100;
                lipSyncBar.style.width = normalizedValue + '%';
                
                // Real-time avatar lip sync animation
                const intensity = normalizedValue / 100;
                
                // Animate mouth overlay
                if (mouthAnimation) {
                    mouthAnimation.style.opacity = intensity * 0.7;
                    mouthAnimation.style.transform = `translateX(-50%) scale(${1 + intensity * 0.4})`;
                }
                
                // Facial animation based on audio levels
                if (avatarImage) {
                    avatarImage.style.filter = `brightness(${1 + intensity * 0.12}) contrast(${1 + intensity * 0.08}) saturate(${1 + intensity * 0.15})`;
                    
                    // Subtle head movement simulation
                    const headMovement = Math.sin(Date.now() / 1000) * intensity * 2;
                    avatarImage.style.transform = `translateY(${headMovement}px) scale(${1 + intensity * 0.02})`;
                }
                
                this.animationFrame = requestAnimationFrame(animate);
            };
            
            this.isAnimating = true;
            animate();
            
        } catch (error) {
            console.error('Lip sync error:', error);
        }
    }
    
    stopLipSync() {
        this.isAnimating = false;
        if (this.animationFrame) {
            cancelAnimationFrame(this.animationFrame);
        }
        
        // Reset all visualizations and animations
        const lipSyncBar = document.getElementById('lipSyncBar');
        const mouthAnimation = document.getElementById('mouthAnimation');
        const avatarImage = document.getElementById('avatarImage');
        
        if (lipSyncBar) {
            lipSyncBar.style.width = '0%';
        }
        
        if (mouthAnimation) {
            mouthAnimation.style.opacity = '0';
            mouthAnimation.style.transform = 'translateX(-50%) scale(1)';
        }
        
        if (avatarImage) {
            avatarImage.style.filter = 'brightness(1) contrast(1) saturate(1)';
            avatarImage.style.transform = 'translateY(0px) scale(1)';
        }
    }
}

// Initialize THE ISP when DOM is ready
document.addEventListener('DOMContentLoaded', () => {
    window.theISP = new TheISP();
});

=== END FILE ===

=== FILE: replit.md ===
# SQUAD ONE - BERYL AGENTIC BUILDER

## Overview

This is a Flask-based web application that provides a professional chat interface for the BERYL AGENTIC BUILDER AI assistant. The application features comprehensive vision tooling, file upload capabilities, and Project Chimera's Build Squad flow visualizer for creating agentic workflows through an intuitive drag-and-drop interface.

## User Preferences

```
Preferred communication style: Simple, everyday language.
UI Framework: Adobe React Spectrum UI with React components for professional interface and client use.
Color Scheme: Black background with teal green (#14B8A6) accents and white text.
Logo Styling: Both "SQUAD ONE" and "BERYL AGENTIC BUILDER" always displayed in dark old gold (#DAA520) thick font (font-weight: 900).
Vision Tooling: Comprehensive file upload suite (images, audio, documents) with drag-and-drop support.
Build Squad Feature: Project Chimera-inspired agentic flow visualizer with custom professional character figurines designed by user.
THE ISP Feature: Real conversation avatar prototyping using multiple AI backends - Gemini 2.5 voice agents, CSM (Conversational Speech Model) for ultra-realistic speech generation, Model Context Protocol (MCP) for 50-100ms GPU latency, HuggingFace streaming, and LiveKit for lip sync and facial animation. Includes programming playground with Monaco editor, GitHub integration, Docker container management, and comprehensive voice interaction capabilities with speech recognition and synthesis.
```

## System Architecture

The application follows a traditional MVC (Model-View-Controller) pattern built on Flask:

### Backend Architecture
- **Framework**: Flask web framework with SQLAlchemy ORM
- **Language Model Integration**: Hugging Face Transformers with the unsloth/Kimi-K2-Instruct-GGUF model
- **Database**: SQLite (development) with support for PostgreSQL via DATABASE_URL environment variable
- **Session Management**: Flask sessions with UUID-based session tracking

### Frontend Architecture
- **Template Engine**: Jinja2 templates
- **UI Framework**: Tailwind CSS with Spectrum UI-inspired design patterns
- **JavaScript**: Vanilla JavaScript for chat functionality with modern ES6+ features
- **Icons**: Lucide Icons (modern successor to Feather Icons)
- **Fonts**: Google Fonts (Inter)
- **Design System**: Custom implementation inspired by Spectrum UI with glass morphism effects

## Key Components

### 1. Application Core (`app.py`)
- Flask application factory pattern
- Database initialization and configuration
- SQLAlchemy setup with declarative base
- ProxyFix middleware for deployment behind reverse proxies
- Environment-based configuration for database and session secrets

### 2. Models (`models.py`)
- **ChatMessage**: Stores conversation history with session tracking
  - Fields: id, session_id, role (user/assistant), content, timestamp
  - Includes serialization method for API responses

### 3. Model Service (`model_service.py`)
- **ModelService**: Handles AI model loading and inference
  - Loads Kimi-K2-Instruct model using Transformers
  - Supports both CUDA and CPU execution
  - Implements text generation pipeline
  - Includes error handling and loading state management

### 4. Routes (`routes.py`)
- **Main Interface** (`/`): Serves the Tailwind CSS chat UI with session initialization
- **React Interface** (`/react`): Serves the React Spectrum UI chat interface
- **Build Squad** (`/build-squad`): Project Chimera agentic flow visualizer interface
- **THE ISP** (`/the-isp`): Avatar conversation prototype with Gemini 2.5 voice agents, CSM speech synthesis, MCP, HuggingFace streaming and LiveKit
- **Deployment Generator** (`/deployment`): One-click deployment configuration generator for 8 platforms
- **MCP Setup** (`/mcp-setup`): Model Context Protocol setup guide for fast local inference
- **API Endpoints**:
  - `/api/gemini-chat`: Gemini 2.5 voice agent responses with natural conversation
  - `/api/csm-speech`: CSM ultra-realistic speech generation with conversational context
  - `/api/csm-status`: CSM model status and availability check
  - `/api/hf-chat`: HuggingFace Llama-3-8B text-based conversations  
  - `/api/github-integration`: GitHub repository and profile access
  - `/api/docker-integration`: Docker Hub container management and deployment
  - `/api/deployment-generator`: One-click deployment configuration generator for multiple platforms
- **Chat API** (`/api/chat`): Handles message processing and AI responses
  - Validates input messages
  - Saves user messages to database
  - Generates AI responses via ModelService
  - Stores conversation history

### 5. Frontend Components
- **HTML Template** (`templates/index.html`): Modern chat interface with Spectrum UI-inspired design using Tailwind CSS
- **React Template** (`templates/react.html`): React-based interface using React Spectrum UI components
- **Build Squad Template** (`templates/build_squad.html`): Interactive agentic flow visualizer with custom character figurines
- **THE ISP Template** (`templates/the_isp.html`): Avatar conversation prototype interface with streaming settings and programming playground
- **Deployment Template** (`templates/deployment.html`): Professional deployment generator interface with platform selection and configuration display
- **CSS Styling**: Tailwind CSS with custom color scheme and glass morphism effects
- **JavaScript** (`static/js/chat.js`): Real-time chat functionality with enhanced UX for Tailwind interface
- **JavaScript** (`static/js/the_isp.js`): Avatar prototype controller with multi-AI backend support, LiveKit integration, and Monaco editor
- **JavaScript** (`static/js/gemini_voice_agent.js`): Comprehensive voice agent with Gemini 2.5, CSM speech synthesis, and multi-platform integration testing
- **Python** (`csm_integration.py`): CSM (Conversational Speech Model) integration for ultra-realistic speech generation with Sesame AI Labs technology
- **React Components**: Native React components with Spectrum UI design system

## Data Flow

1. **User Interaction**: User types message in web interface
2. **Frontend Processing**: JavaScript validates input and sends POST request to `/api/chat`
3. **Backend Processing**: 
   - Flask route receives message
   - Saves user message to database
   - Calls ModelService to generate AI response
   - Saves AI response to database
   - Returns response to frontend
4. **UI Update**: JavaScript updates chat interface with new messages

## External Dependencies

### Python Packages
- **Flask**: Web framework and routing
- **Flask-SQLAlchemy**: Database ORM
- **Transformers**: Hugging Face model loading and inference
- **PyTorch**: Deep learning framework for model execution
- **Werkzeug**: WSGI utilities and middleware

### Frontend Dependencies (CDN)
- **Tailwind CSS**: Utility-first CSS framework for rapid UI development (main interface)
- **React & ReactDOM**: Core React libraries for component-based interface
- **Spectrum CSS**: Adobe's design system CSS for React interface
- **Lucide Icons**: Modern icon library with React-like API (Tailwind interface)
- **Google Fonts**: Typography (Inter font family)
- **Design System**: Both Spectrum UI-inspired (Tailwind) and native Spectrum UI (React)

### AI Models & Voice Technology
- **unsloth/Kimi-K2-Instruct-GGUF**: Language model from Hugging Face
- **CSM (Conversational Speech Model)**: Ultra-realistic speech generation from Sesame AI Labs
  - CSM-1B model with Llama-3.2-1B tokenizer
  - Mimi audio codec for 24kHz high-quality audio
  - Professional watermarking for AI transparency
- **LiveKit Voice AI**: Real-time conversational agents
  - STT-LLM-TTS pipeline with voice activity detection
  - Turn detection and noise cancellation
  - Multi-provider integration (OpenAI, Deepgram, Cartesia)
- Supports both GPU (CUDA) and CPU inference
- Uses GGUF format for efficient loading

## Deployment Strategy

### Configuration
- Environment variables for sensitive data (DATABASE_URL, SESSION_SECRET)
- Support for both development (SQLite) and production (PostgreSQL) databases
- ProxyFix middleware for deployment behind load balancers/reverse proxies

### Database Management
- Automatic table creation on startup
- Connection pooling with health checks (pool_pre_ping)
- Connection recycling every 300 seconds

### Model Loading
- Automatic model initialization on startup
- Device detection (CUDA/CPU) with appropriate memory management
- Error handling for model loading failures

### Development vs Production
- Debug mode enabled for development
- Configurable host and port settings
- Logging configured at DEBUG level for troubleshooting

The application is designed to be easily deployable on platforms like Replit, with minimal configuration required for basic functionality.

## Project Artifacts & Deliverables

### Core Features Implemented
1. **SQUAD ONE Chat Interface** - Professional conversational AI with dark old gold branding
2. **Build Squad Flow Visualizer** - Project Chimera-inspired agentic workflow builder
3. **THE ISP Voice Avatar System** - Advanced multi-modal conversation prototype
4. **Deployment Generator** - One-click deployment configurations for 8 platforms

### Advanced Voice AI Integration
- **CSM (Conversational Speech Model)** - Ultra-realistic speech synthesis from Sesame AI Labs
- **LiveKit Voice AI** - Real-time conversational agents with STT-LLM-TTS pipeline
- **Gemini 2.5 Voice Agents** - Natural conversation with context awareness
- **Professional Audio Pipeline** - 24kHz quality with AI watermarking

### Multi-Platform Deployment Support
- **Replit** - Complete .replit configuration with Nix packages
- **Docker** - Multi-stage builds with health checks and PostgreSQL
- **Vercel** - Serverless deployment with environment variables
- **Heroku** - Procfile with PostgreSQL addon integration
- **Railway** - Nixpacks configuration with auto-deployment
- **Render** - Web service with health check monitoring
- **Fly.io** - Global deployment with auto-rollback
- **DigitalOcean** - App Platform with managed database

### API Integrations
- **GitHub Integration** - Repository access and profile management
- **Docker Hub Integration** - Container management and deployment
- **HuggingFace Streaming** - Real-time model inference
- **Multi-Provider AI** - OpenAI, Anthropic, Google Gemini support

### Professional UI Components
- **Tailwind CSS Design System** - Custom color scheme with glass morphism
- **React Spectrum UI** - Adobe's professional component library
- **Monaco Editor** - Full programming environment integration
- **Responsive Design** - Landscape format optimized for professional use

### Security & Production Features
- **Environment Variable Management** - Secure API key handling
- **Session Management** - UUID-based conversation tracking
- **Database Abstraction** - SQLite development, PostgreSQL production
- **Error Handling** - Comprehensive logging and diagnostics
- **Health Checks** - Application monitoring and status reporting

### Technical Architecture
- **Flask Backend** - Python 3.11 with SQLAlchemy ORM
- **Modern Frontend** - Vanilla JavaScript with ES6+ features
- **Database Layer** - Flexible SQLite/PostgreSQL support
- **AI Model Integration** - Multiple provider support with fallbacks
- **Voice Processing** - Advanced audio synthesis and recognition

### Deployment Ready Status
‚úÖ All compilation issues resolved - Application running stable
‚úÖ Complete platform configuration files generated for 8 platforms
‚úÖ Environment variable templates provided with secure handling
‚úÖ Professional deployment documentation with step-by-step guides
‚úÖ One-click deployment capability with automated config generation
‚úÖ Production-ready architecture with optimized dependencies
‚úÖ Gemini AI service integration active and operational
‚úÖ Voice AI pipeline ready for real-time conversations
‚úÖ Professional UI with dark old gold branding implemented
‚úÖ Multi-modal interaction capabilities fully functional

### Recent Deployment Fixes (January 2025)
- **Dependency Conflicts Resolved**: Removed problematic PyTorch/Transformers dependencies causing build failures
- **Stable AI Backend**: Configured reliable Gemini AI service for production deployment
- **Optimized Package Management**: Streamlined dependencies for reliable cross-platform deployment
- **Enhanced Error Handling**: Improved fallback systems and graceful degradation
- **Production Optimization**: Removed development-only packages that caused deployment conflicts

### Voice System Enhancements (January 2025)
- **Browser Speech Synthesis**: Implemented reliable voice functionality using Web Speech API
- **Real-time Lip Sync**: Added visual mouth animation synchronized with speech output
- **CSM-1B Integration Ready**: Sesame CSM-1B ultra-realistic voice model integration prepared
- **Dual Voice Options**: Gold button for browser voice, brown button for CSM-1B (when available)
- **Professional Audio Pipeline**: 24kHz quality with automatic fallback systems
- **Conversation System Fixed**: Removed environment initialization requirement for immediate voice access

### Critical Lip Sync Repair Solution (January 2025) - IMPLEMENTED
- **Root Cause Identified**: Face-anchor mis-scaling and 256√ó256 crop losing mouth region in wav2lip-2
- **Critical Issue**: "The mouth simply isn't inside the square crop that the lip-syncer is expecting"
- **Problem**: 45% forehead + 55% upper lip = top lip flicker only, no lower lip motion
- **Solution Applied**: FFmpeg crop filter with precise coordinates (h*0.45:0.75, w*0.35:0.65)
- **Implementation**: `crop=iw*0.30:ih*0.30:iw*0.35:ih*0.45,scale=256:256` in instant_lipsync.py
- **Result**: Mouth properly positioned INSIDE the 256√ó256 square for accurate lip sync
- **Status**: COMPLETED - Both instant and advanced solutions implemented

### Backup Instant Lip Sync (Zero-Coding Solution)
- **Files Added**: mouth_roi.jpg and sync.mp4 from Sesame CSM-1B utilities
- **11-Line Solution**: instant_lipsync.py provides immediate lip sync using FFmpeg only
- **API Endpoints**: /instant-lipsync for immediate results, /lip-sync-repair for advanced processing
- **Deployment Ready**: Works with any 22kHz 16-bit WAV, outputs 256√ó256 MP4 overlay
- **Fallback Strategy**: Zero dependencies, instant deployment, adjustable with drag slider

### Live Feature Status
üü¢ **Core Application**: Running stable on port 5000
üü¢ **Chat Interface**: Operational with Gemini AI backend
üü¢ **Build Squad**: Ready for agentic workflow creation
üü¢ **THE ISP Voice System**: Active with multi-AI integration
üü¢ **Deployment Generator**: Functional with 8-platform support
üü¢ **API Endpoints**: All services responding correctly
üü¢ **Database**: SQLAlchemy ORM with session management active
üü¢ **Security**: Environment variable handling and session tracking operational

The project represents a comprehensive AI agent builder platform with state-of-the-art voice capabilities, professional deployment options, and enterprise-ready features. Successfully deployed and operational as of January 2025.

=== END FILE ===

=== FILE: pyproject.toml ===
[project]
name = "repl-nix-workspace"
version = "0.1.0"
description = "Add your description here"
requires-python = ">=3.11"
dependencies = [
    "email-validator>=2.2.0",
    "flask>=3.1.1",
    "flask-sqlalchemy>=3.1.1",
    "google-genai",
    "google-generativeai",
    "gunicorn>=23.0.0",
    "psycopg2-binary>=2.9.10",
    "requests>=2.32.0",
]

[[tool.uv.index]]
explicit = true
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"

[tool.uv.sources]
AA-module = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ABlooper = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
AnalysisG = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
AutoRAG = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
BERTeam = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
BxTorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
Byaldi = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
CALM-Pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
COPEX-high-rate-compression-quality-metrics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
CityLearn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
CoCa-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
CoLT5-attention = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ComfyUI-EasyNodes = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
Crawl4AI = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
DALL-E = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
DI-toolkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
DatasetRising = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
DeepCache = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
DeepMatter = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
Draugr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ESRNN = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
En-transformer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ExpoSeq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
FLAML = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
FSRS-Optimizer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
GANDLF = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
GQLAlchemy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
GhostScan = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
GraKeL = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
HEBO = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
IOPaint = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ISLP = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
InvokeAI = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
JAEN = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
KapoorLabs-Lightning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
LightAutoML = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
LingerGRN = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
MMEdu = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
MRzeroCore = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
Modeva = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
NeuralFoil = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
NiMARE = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
NinjaTools = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
OpenHosta = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
OpenNMT-py = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
POT = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
PVNet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
PaLM-rlhf-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
PepperPepper = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
PiML = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
Poutyne = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
QNCP = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
RAGatouille = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
RareGO = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
RealtimeSTT = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
RelevanceAI-Workflows-Core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
Resemblyzer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ScandEval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
Simba-UW-tf-dev = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
SwissArmyTransformer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
TPOT = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
TTS = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
TorchCRF = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
TotalSegmentator = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
UtilsRL = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
WhisperSpeech = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
XAISuite = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
a-unet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
a5dev = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
accelerate = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
accelerated-scan = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
accern-xyme = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
achatbot = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
acids-rave = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
actorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
acvl-utils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
adabelief-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
adam-atan2-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
adan-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
adapters = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
admin-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
adtoolbox = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
adversarial-robustness-toolbox = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aeiou = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aeon = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
africanwhisper = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ag-llama-api = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
agentdojo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
agilerl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ai-edge-torch-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ai-parrot = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ai-python = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ai-transform = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ai2-olmo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ai2-olmo-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ai2-tango = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aicmder = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aider-chat = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aider-chat-x = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aif360 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aihwkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aimodelshare = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
airllm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
airtestProject = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
airunner = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aisak = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aislib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aisquared = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aistore = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aithree = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
akasha-terminal = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alibi = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alibi-detect = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alignn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
all-clip = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
allennlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
allennlp-models = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
allennlp-pvt-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
allophant = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
allosaurus = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aloy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alpaca-eval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alphafold2-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alphafold3-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alphamed-federated = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
alphawave = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
amazon-braket-pennylane-plugin = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
amazon-photos = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
anemoi-graphs = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
anemoi-models = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
anomalib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
apache-beam = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
apache-tvm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aperturedb = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aphrodite-engine = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aqlm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
arcAGI2024 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
archisound = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
argbind = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
arize = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
arm-pytorch-utilities = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
array-api-compat = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
arus = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
assert-llm-tools = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
asteroid = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
asteroid-filterbanks = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
astra-llm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
astrovision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
atomate2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
attacut = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
audio-diffusion-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
audio-encoders-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
audio-separator = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
audiocraft = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
audiolm-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
auralis = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
auraloss = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
auto-gptq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
autoawq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
autoawq-kernels = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
"autogluon.multimodal" = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
"autogluon.tabular" = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
"autogluon.timeseries" = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
autotrain-advanced = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
avdeepfake1m = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
aws-fortuna = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ax-platform = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
azureml-automl-dnn-vision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
azureml-contrib-automl-dnn-forecasting = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
azureml-evaluate-mlflow = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
azureml-metrics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
azureml-train-automl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
b2bTools = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
backpack-for-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
balrog-nle = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
batch-face = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
batchalign = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
batchgeneratorsv2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
batchtensor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bbrl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
benchpots = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bent = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bert-score = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bertopic = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bertviz = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bestOf = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
betty-ml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
big-sleep = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bigdl-core-cpp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bigdl-core-npu = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bigdl-llm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bigdl-nano = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
"bioimageio.core" = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bitfount = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bitsandbytes = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bittensor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bittensor-cli = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
blackboxopt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
blanc = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
blindai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
bm25-pt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
boltz = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
botorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
boxmot = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
brainchain = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
braindecode = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
brevitas = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
briton = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
browsergym-visualwebarena = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
buzz-captions = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
byotrack = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
byzerllm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
c4v-py = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
calflops = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
came-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
camel-ai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
camel-tools = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cannai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
captum = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
carte-ai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
carvekit-colab = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
catalyst = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
causalml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
causalnex = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
causy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cbrkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cca-zoo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cdp-backend = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cellacdc = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cellfinder = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cellpose = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cellxgene-census = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
chattts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
chemprop = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
chgnet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
chitra = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
circuitsvis = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cjm-yolox-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clarinpl-embeddings = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
class-resolver = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
classifier-free-guidance-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
classiq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
classy-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clean-fid = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cleanvision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clip-anytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clip-benchmark = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clip-by-openai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clip-interrogator = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clip-retrieval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cltk = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clu = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
clusterops = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cnocr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cnstd = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
coba = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cofi = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
colbert-ai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
colpali-engine = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
compel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
composabl-ray = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
composabl-ray-dev = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
composabl-train = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
composabl-train-dev = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
composer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
compressai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
compressed-tensors = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
compressed-tensors-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
concrete-python = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
confit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
conformer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
contextualSpellCheck = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
continual-inference = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
controlnet-aux = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
convokit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
coola = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
coqui-tts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
coqui-tts-trainer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
craft-text-detector = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
creme = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
crocodile = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
crowd-kit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cryoSPHERE = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
csle-common = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
csle-system-identification = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ctgan = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
curated-transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cut-cross-entropy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cvat-sdk = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
cybertask = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
d3rlpy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dalle-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dalle2-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
danila-lib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
danling = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
darts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
darwin-py = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
data-gradients = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
datachain = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dataclass-array = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dataeval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
datarobot-drum = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
datarobotx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
datasets = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
datumaro = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dctorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deep-utils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepchecks = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepchem = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepctr-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepecho = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepepochs = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepforest = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deeplabcut = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepmd-kit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepmultilingualpunctuation = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepparse = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deeprobust = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepsparse = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepsparse-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
deepspeed = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
denoising-diffusion-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
descript-audio-codec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
descript-audiotools = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
detecto = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
detoxify = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dgenerate = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dghs-imgutils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dgl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dialogy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dice-ml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
diffgram = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
diffq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
diffusers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
distilabel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
distrifuser = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dnikit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
docarray = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
doclayout-yolo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
docling-ibm-models = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
docquery = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
domino-code-assist = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dreamsim = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dropblock = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
druida = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
dvclive = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
e2-tts-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
e2cnn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
e3nn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
easyocr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ebtorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ecallisto-ng = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
edsnlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
effdet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
einx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
eir-dl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
eis1600 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
eland = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ema-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
embedchain = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
enformer-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
entmax = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
esm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
espaloma-charge = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
espnet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
etils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
etna = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
evadb = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
evalscope = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
evaluate = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
exllamav2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
extractable = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
face-alignment = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
facenet-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
facexlib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fair-esm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fairseq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fairseq2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fairseq2n = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
faker-file = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
farm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fast-bert = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fast-pytorch-kmeans = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fastai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fastcore = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fastestimator-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fasttreeshap = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fedml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
felupe = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
femr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fft-conv-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fickling = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fireworks-ai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flair = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flashrag-dev = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flax = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flexgen = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flgo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flopth = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flowcept = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flytekitplugins-kfpytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
flytekitplugins-onnxpytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fmbench = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
focal-frequency-loss = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
foldedtensor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fractal-tasks-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
freegenius = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
freqtrade = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
fschat = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
funasr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
functorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
funlbm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
funsor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
galore-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
garak = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
garf = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gateloop-transformer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
geffnet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
genutility = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gfpgan = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gigagan-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gin-config = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
glasflow = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gliner = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gluonts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gmft = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
google-cloud-aiplatform = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gpforecaster = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gpt3discord = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gpytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
grad-cam = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
graph-weather = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
graphistry = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gravitorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gretel-synthetics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gsplat = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
guardrails-ai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
guidance = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
gymnasium = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hanlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
happytransformer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hbutils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
heavyball = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hezar = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hf-deepali = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hf-doc-builder = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
higher = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hjxdl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hkkang-utils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hordelib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hpsv2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
huggingface-hub = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hummingbird-ml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hvae-backbone = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hya = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
hypothesis-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ibm-metrics-plugin = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ibm-watson-machine-learning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ibm-watsonx-ai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
icetk = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
icevision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
iden = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
idvpackage = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
iglovikov-helper-functions = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
imagededup = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
imagen-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
imaginAIry = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
img2vec-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
incendio = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
inference = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
inference-gpu = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
infinity-emb = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
info-nce-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
infoapps-mlops-sdk = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
instructlab = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
instructlab-dolomite = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
instructlab-eval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
instructlab-sdg = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
instructlab-training = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
invisible-watermark = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
iobm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ipex-llm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
iree-turbine = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
irisml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
irisml-tasks-azure-openai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
irisml-tasks-torchvision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
irisml-tasks-training = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
item-matching = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ivadomed = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
jaqpotpy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
jina = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
judo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
junky = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
k-diffusion = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
k1lib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
k2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kappadata = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kappamodules = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
karbonn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kats = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kbnf = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kedro-datasets = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
keybert = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
keytotext = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
khoj = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kiui = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
konfuzio-sdk = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kornia = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kornia-moons = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kraken = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kwarray = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
kwimage = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
labml-nn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lagent = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
laion-clap = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lale = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lama-cleaner = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lancedb = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
langcheck = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
langkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
langroid = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
langtest = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
layoutparser = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ldp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
leafmap = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
leap-ie = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
leibniz = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
leptonai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
letmedoit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lhotse = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lib310 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
libpecos = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
librec-auto = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
libretranslate = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
liger-kernel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
liger-kernel-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightning-bolts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightning-fabric = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightning-habana = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightning-lite = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightrag = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightweight-gan = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lightwood = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
linear-attention-transformer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
linear-operator = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
linformer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
linformer-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
liom-toolkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lion-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lit-nlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
litdata = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
litelama = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
litgpt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llama-index-embeddings-adapter = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llama-index-embeddings-clip = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llama-index-embeddings-instructor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llama-index-llms-huggingface = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llama-index-postprocessor-colbert-rerank = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llm-blender = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llm-foundry = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llm-guard = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llm-rs = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llm2vec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llmcompressor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llmlingua = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
llmvm-cli = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lm-eval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lmdeploy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lmms-eval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
local-attention = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lovely-tensors = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lpips = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
lycoris-lora = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mace-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
magic-pdf = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
magicsoup = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
magvit2-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
maite = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
manga-ocr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
manifest-ml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
manipulation = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
marker-pdf = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
matgl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
med-imagetools = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
medaka = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
medcat = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
medmnist = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
megablocks = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
megatron-energon = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
memos = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
meshgpt-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
metatensor-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mflux = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mia-vgg = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
miditok = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
minari = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
minicons = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ml2rt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mlagents = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mlbench-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mlcroissant = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mlpfile = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mlx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mlx-whisper = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mmaction2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mmengine = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mmengine-lite = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mmocr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mmpose = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mmsegmentation = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
modeci-mdf = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
model2vec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
modelscope = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
modelspec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
monai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
monai-weekly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
monotonic-alignment-search = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
monty = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mosaicml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mosaicml-streaming = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
moshi = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mteb = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
mtmtrain = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
multi-quantization = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
myhand = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nGPT-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
naeural-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
napari = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
napatrackmater = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nara-wpe = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
natten = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nbeats-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nebulae = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nemo-toolkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
neptune = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
neptune-client = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nerfacc = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nerfstudio = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nessai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
netcal = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
neural-rag = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
neuralforecast = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
neuralnets = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
neuralprophet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
neuspell = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nevergrad = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nexfort = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nimblephysics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nirtorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nkululeko = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nlptooltest = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nnAudio = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nnodely = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nnsight = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nnunetv2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
noisereduce = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nonebot-plugin-nailongremove = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nowcasting-dataloader = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nowcasting-forecast = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nshtrainer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nuwa-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nvflare = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
nvidia-modelopt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ocf-datapipes = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ocnn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ogb = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ohmeow-blurr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
olive-ai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
omlt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ommlx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
onediff = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
onediffx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
onnx2pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
onnx2torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
opacus = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
open-clip-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
open-flamingo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
open-interpreter = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openbb-terminal-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openmim = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openparse = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openunmix = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openvino-dev = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openvino-tokenizers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openvino-xai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
openwakeword = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
opt-einsum-fx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optimum = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optimum-habana = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optimum-intel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optimum-neuron = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optimum-quanto = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optree = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optuna = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optuna-dashboard = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
optuna-integration = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
oracle-ads = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
orbit-ml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
otx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
outetts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
outlines = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
outlines-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
paddlenlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pai-easycv = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pandasai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
panns-inference = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
patchwork-cli = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
peft = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pegasuspy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pelutils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
penn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
perforatedai-freemium = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
performer-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
petastorm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pfio = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pgmpy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
phenolrs = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
phobos = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pi-zero-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pinecone-text = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
piq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pix2tex = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pix2text = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pnnx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
policyengine-us-data = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
polyfuzz = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pomegranate = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
positional-encodings = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
prefigure = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
product-key-memory = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ptflops = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ptwt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pulser-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
punctuators = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
py2ls = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyabsa = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
"pyannote.audio" = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyawd = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyclarity = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pycox = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyfemtet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyg-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pygrinder = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyhealth = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyhf = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyiqa = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pykeen = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pykeops = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pylance = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pylineaGT = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pymanopt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pymde = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pypots = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyqlib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyqtorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyro-ppl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pysentimiento = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyserini = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pysr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pythainlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
python-doctr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-fid = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-forecasting = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-ignite = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-kinematics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-lightning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-lightning-bolts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-metric-learning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-model-summary = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-msssim = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-pfn-extras = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-pretrained-bert = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-ranger = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-seed = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-tabnet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-tabular = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-toolbelt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-transformers-pvt-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-triton-rocm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-warmup = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch-wavelets = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch_optimizer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorch_revgrad = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorchcv = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pytorchltr2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyvene = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
pyvespa = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
qianfan = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
qibo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
qiskit-machine-learning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
qtorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
quanto = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
quick-anomaly-detector = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rastervision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rastervision-pytorch-backend = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rastervision-pytorch-learner = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ray-lightning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rclip = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
realesrgan = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
recbole = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
recommenders = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
redcat = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
reformer-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
regex-sampler = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
replay-rec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rerankers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
research-framework = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
resemble-enhance = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
resnest = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rf-clip = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rf-groundingdino = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rfconv = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rich-logger = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ring-attention-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rltrade-test = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rotary-embedding-torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rsp-ml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
rust-circuit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
s2fft = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
s3prl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
s3torchconnector = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
saferx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
safetensors = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sagemaker-huggingface-inference-toolkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sagemaker-ssh-helper = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
salesforce-lavis = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
salesforce-merlion = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
samv2 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
scib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
scib-metrics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
scvi-tools = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sdmetrics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
secretflow = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
segment-anything-hq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
segment-anything-py = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
segmentation-models-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
self-rewarding-lm-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
semantic-kernel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
semantic-router = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
senselab = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sent2vec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sentence-transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sequence-model-train = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
serotiny = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sevenn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sglang = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
shap = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
silero-api-server = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
silero-vad = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
silicondiff-npu = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
simclr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
simple-lama-inpainting = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sinabs = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sixdrepnet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
skforecast = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
skorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
skrl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
skt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sktime = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sktmls = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
slangtorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
smartnoise-synth = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
smashed = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
smplx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
smqtk-descriptors = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
smqtk-detection = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
snntorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
snorkel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
snowflake-ml-python = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
so-vits-svc-fork = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sonusai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sony-custom-layers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sotopia = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spacr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spacy-curated-transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spacy-experimental = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spacy-huggingface-pipelines = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spacy-llm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spacy-transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
span-marker = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spandrel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spandrel-extra-arches = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sparrow-python = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spatialdata = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
speechbrain = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
speechtokenizer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spikeinterface = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spikingjelly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spotiflow = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spotpython = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
spotriver = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
squirrel-core = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
stable-baselines3 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
stable-diffusion-sdkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
stable-ts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
stanford-stk = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
stanfordnlp = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
stanza = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
startorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
streamtasks = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
struct-eqtable = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
stylegan2-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
supar = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
super-gradients = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
super-image = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
superlinked = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
supervisely = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
surya-ocr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
svdiff-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
swarm-models = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
swarmauri = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
swarms-memory = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
swebench = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
syft = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
sympytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
syne-tune = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
synthcity = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
t5 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tab-transformer-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tabpfn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
taming-transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
taming-transformers-rom1504 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
taskwiz = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tbparse = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tecton = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tensor-parallel = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tensorcircuit-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tensordict = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tensordict-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tensorizer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tensorrt-llm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
texify = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
text2text = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
textattack = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tfkit = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
thepipe-api = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
thinc = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
thingsvision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
thirdai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
thop = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tianshou = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tidy3d = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
timesfm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
timm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tipo-kgen = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tmnt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
toad = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tomesd = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
top2vec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-audiomentations = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-dct = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-delaunay = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-directml = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-ema = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-encoding = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-fidelity = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-geometric = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-geopooling = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-harmonics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-kmeans = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-lr-finder = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-max-mem = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-npu = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-optimi = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-optimizer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-ort = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-pitch-shift = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-ppr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-pruning = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-snippets = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-stoi = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-struct = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torch-tensorrt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchani = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchattacks = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchaudio = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchbiggraph = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchcam = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchcde = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchcfm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchcrepe = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchdata = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchdatasets-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchdiffeq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchdyn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchestra = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torcheval = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torcheval-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchextractor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchfcpe = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchfun = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchfunc-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchgeo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchgeometry = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchio = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchjpeg = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchlayers-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchmeta = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchmetrics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchmocks = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchpack = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchpippy = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchpq = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchprofile = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchquantlib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchrec = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchrec-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchrec-nightly-cpu = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchrl = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchrl-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchscale = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchsde = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchseg = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchserve = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchserve-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchsnapshot-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchsr = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchstain = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchsummaryX = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchtext = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchtnt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchtnt-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchtyping = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchutil = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchvinecopulib = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchvision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchviz = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchx-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
torchxrayvision = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
totalspineseg = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tracebloc-package-dev = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
trainer = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
transformer-engine = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
transformer-lens = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
transformer-smaller-training-vocab = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
transformers-domain-adaptation = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
transfusion-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
transparent-background = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
treescope = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
trolo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tsai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tslearn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ttspod = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
txtai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
tyro = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
u8darts = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
uhg = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
uitestrunner-syberos = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ultimate-rvc = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ultralytics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
ultralytics-thop = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unav = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unbabel-comet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
underthesea = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unfoldNd = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unimernet = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unitorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unitxt = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unsloth = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unsloth-zoo = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unstructured = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
unstructured-inference = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
utilsd = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
v-diffusion-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vIQA = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vectice = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vector-quantize-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vectorhub-nightly = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
versatile-audio-upscaler = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vertexai = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vesin = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vgg-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
video-representations-extractor = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
viser = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vision-datasets = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
visionmetrics = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
visu3d = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vit-pytorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
viturka-nn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vllm = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vllm-flash-attn = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vocos = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vollseg = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
vtorch = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
wavmark = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
wdoc = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
whisper-live = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
whisper-timestamped = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
whisperx = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
wilds = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
wordllama = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
worker-automate-hub = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
wxbtool = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
x-clip = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
x-transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
xaitk_saliency = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
xformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
xgrammar = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
xinference = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
xtts-api-server = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
yolo-poser = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
yolov5 = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
yolov7-package = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
yta-general-utils = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
zensvi = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
zetascale = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
zuko = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]
transformers = [{ index = "pytorch-cpu", marker = "platform_system == 'Linux'" }]


=== END FILE ===

